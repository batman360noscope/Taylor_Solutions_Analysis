% --------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% --------------------------------------------------------------

\documentclass[12pt]{book}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsthm,amssymb,hyperref}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}

\hypersetup{
    bookmarks=true,
    unicode=true,
    pdftitle={Michael Senter}
}


\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{question}[2][Question]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

\newcommand{\prb}[1]{\textbf{Exercise #1.}}

\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

\begin{document}

% --------------------------------------------------------------
%                         Start here
% --------------------------------------------------------------

\title{Solutions Manual for Joseph Taylor's \\ \emph{Foundations of Analysis}}%replace X with the appropriate number
\author{Michael Senter}
\date{}

\maketitle

\part{Single Variable - 3210}


\chapter{The Real Numbers}

\section{Sets and Functions}

\begin{exercise}{1.1.1} %You can use theorem, exercise, problem, or question here.  Modify x.yz to be whatever number you are proving
If $a, b \in \mathbb{R}$ and $a < b$, give a description in set theory notation for each of the intervals $(a, b)$, $[a, b]$, $[a, b)$, and $(a, b]$ (see Example 1.1.1).
\begin{align*}
	(a,b) &= \{x \in \mathbb{R} : a < x < b\} \\
	[a,b] &= \{x \in \mathbb{R} : a \leq x \leq b\} \\
	[a,b) &= \{x \in \mathbb{R} : a \leq x < b\} \\
	(a,b] &= \{x \in \mathbb{R} : a < x \leq b\}.
\end{align*}
\end{exercise}

\begin{theorem}{1.1.2}
If $A$, $B$, and $C$ are sets, then $A  \cap (B \cup C) = (A \cap B) \cup (A \cap C)$.
\end{theorem}
\begin{proof}
If  $x \in A  \cap (B \cup C)$, then $x \in A$ and $x \in (B \cup C)$. Thus, either $x \in B$ or $x \in C$. Thus, $x \in A \cap B$ or in $x \in A \cap C$. Thus, if $x \in A  \cap (B \cup C)$, then $x \in (A \cap B) \cup (A \cap C)$. \\
If an $x \in (A \cap B) \cup (A \cap C)$, then either $x \in (A \cap B)$ or $x \in (A \cap C)$. This means that surely $x \in A$, and also that $x \in B \cup C$. Hence, if $x \in (A \cap B) \cup (A \cap C)$, then $x \in A  \cap (B \cup C)$.\\
Therefore, $A  \cap (B \cup C) = (A \cap B) \cup (A \cap C)$.
\end{proof}


\begin{exercise}{1.1.3}
\emph{Solved in Class}
\end{exercise}


\begin{question}{1.1.5}
What is the intersection of all the closed intervals containing the open interval $(0, 1)$? Justify your answer. \\

Let $\mathcal{A}$ denote the set of all sets such that $(0,1) \in \mathcal{A_i}$.  The intersection of all closed intervals is denoted $\bigcap \mathcal{A}$. It is defined as
\begin{align*}
	\bigcap \mathcal{A} = \{x: x \in A \text{ } \forall A \in \mathcal{A}\}.
\end{align*}
In other words, we are looking for some set $A$ such that $A$ is a subset of every other set in $\mathcal{A}$. This set is $A = \{x: 0 < x < 1\}$. Consider any other subset $C$ of $\mathcal{A}$. If $C \neq A$, then necessarily there must exist an element $x$ such that $x \in C$ and $x \notin A$, showing that $A \subset C$ but $C \nsubseteq A$. Since $x$ is not in every subset of $\mathcal{A}$, $x \notin \bigcap \mathcal{A}$.
\end{question}

\begin{question}{1.1.6}
What is the union of all of the closed intervals contained in the open
interval $(0, 1)$? Justify your answer. \\

Let $\mathcal{A}$ be the set containing all sets containing $(0,1)$ as a subset. The union of all these sets is denoted by $\bigcup \mathcal{A}$. An object $x$ is an element of $\bigcup \mathcal{A}$ if there exists some set $C \subset \mathcal{A}$ such that $x \in C$. We need to consider two cases: either the object $x \leq 0$ or $ 1 \leq x$. The case of $0 < x < 1$ is trivial. For any $x$ such that $1\leq x$ we can create a set $C$ such that $C = \{ y: 0 < y < x \}$. This since $1 \leq x$, it is guaranteed that $C \subset \mathcal{A}$. The case of $x \leq 0$ is analogous. Hence, $\bigcup \mathcal{A} = (- \infty, \infty)$.
\end{question}

\begin{problem}{1.1.7}
If A is a collection of subsets of a set X, formulate and prove a theorem
like Theorem 1.1.5 (\emph{from book numbering}) for the intersection and union of A.

\begin{theorem}{1.1.7}
	Let $\mathcal{A}$ be a collection of subsets $A_1, A_2,...,A_n$ of some set $X$. Then $( \bigcup \mathcal{A})^c = A_1^c \cap A_2^c \cap ... \cap A_n^c$ and $( \bigcap \mathcal{A})^c = A_1^c  \cup A_2^c \cup ... \cup A_n^c$.
\end{theorem}

\begin{proof}
This is a generalization of DeMorgan's law, proved in the book. We begin with the statement $( \bigcup \mathcal{A})^c = A_1^c \cap A_2^c \cap ... \cap A_n^c$.	We can rewrite $ (\bigcup \mathcal{A})^c$ as $( A_1 \cup A_2 \cup ... \cup A_n)^c$. We can then sub-partition this collection of unions into a collection of two unions, as such:
\begin{align*}
	(\bigcup \mathcal{A})^c = [ A_1 \cup ( A_2 \cup ... \cup A_n) ]^c
\end{align*}
Then we will refer to $A_2 \cup ... \cup A_n$ as $B$. We can then rewrite the above as $(A_1 \cup B)^c$, for which DeMorgans laws apply. Thus, we write $(A_1 \cup B)^c =  A_1^c \cap B^c = A_1^c \cap ( A_2 \cup ... \cup A_n)^c$. As next step, we sub partition B into two sets, as such
\begin{align*}
	( A_2 \cup ... \cup A_n)^c = [ A_2 \cup (A_3 \cup .... \cup A_n)]^c
\end{align*}
Then DeMorgan's laws apply again as above, and we can write $[ A_2 \cup (A_3 \cup .... \cup A_n)]^c = A_2^c \cap (A_3 \cup ... \cup A_n)^c$. Since intersections and unions are associative, we can then write
\begin{align*}
	(\bigcup \mathcal{A})^c = (A_1^c \cap (A_2^c \cap (A_3 \cup ... \cup A_n)^c)) = A_1^c \cap A_2^c \cap (A_3 \cup ... \cup A_n)^c
\end{align*}
We continue an inductive application of DeMorgan's laws as outlined above, until we see that $( \bigcup \mathcal{A})^c = A_1^c \cap A_2^c \cap ... \cap A_n^c$ \\~\\
The other proof is analogous, requiring a sub-partition of the collection of intersections and rewriting them into series of intersections of two sets to which DeMorgan's laws apply.
\end{proof}

\end{problem}

\begin{problem}{1.1.8}
Which of the following functions $f : \mathbb{R} \to \mathbb{R}$ are one to one and which ones are onto. Justify your answer. \\
(a) $f (x) = x^2$ ; This function is neither onto, nor one-to-one. It is not onto, since there is no $x$ such that $f(x) < 0$. It is not one-to-one since $f(x)=f(-x)$ for all $x \in \mathbb{R}$. \\
(b) $f (x) = x^3$ ; This function is both one-to-one and onto. It is one-to-one since there $f(x) \neq f(y)$ for all $x,y$ such that $x \neq y$. It is onto, as for any $y \in \mathbb{R}$, there exists an $x \in \mathbb{R}$ such that $f(x) = y$. \\
(c) $f (x) = e^x$ This function is one-to-one, but not onto. It is one-to-one, for $f(x) \neq f(y)$ for all $x,y \in \mathbb{R}$ such that $x \neq y$. It fails to be onto since there exists no $x$ such that $f(x)<0$ for any $x \in \mathbb{R}$.
\end{problem}

\begin{theorem}{1.1.9}
If $f : A \to B$ is a function and $E$ and $F$ are subsets of $B$, then $f^{-1} (E \cap F ) = f^{-1} (E) \cap f^{-1} (F )$.
\end{theorem}

\begin{proof}
If $x \in f^{-1} (E \cap F )$, then $f(x) \in E \cap F$. This means that $f(x)$ is both in $E$ as well as in $F$. If $f(x) \in E$, then $x \in f^{-1}(E)$. If $f(x) \in F$, then $x \in f^{-1}(F)$. Since $f(x)$ is in both $E$ and $F$, $x$ is in $f^{-1} (E \cap F)$. \\
Assume $x$ is in $f^{-1} (E) \cap f^{-1} (F )$. Then, $x \in f^{-1} (E)$ as well as $x \in f^{-1} (F)$. If $x \in f^{-1} (E)$, then $f(x) \in E$. If $x \in f^{-1}(F)$, then $f(x) \in F$. Since $x$ is both in $f^{-1} (E)$ as well as $f^{-1} (F)$, we know that $f(x) \in E \cap F$. This implies that $x \in f^{-1} (E \cap F)$. \\
Since every $x \in f^{-1} (E \cap F )$ implies that $x \in f^{-1} (E) \cap f^{-1} (F )$ and vice versa, it is true that  $f^{-1} (E \cap F ) = f^{-1} (E) \cap f^{-1} (F )$.
\end{proof}

\begin{theorem}{1.1.10}
If $f : A \to B$ is a function and $E$ and $F$ are subsets of $B$, then $f^{-1} (E \backslash F ) = f^{-1} (E) \backslash f^{-1} (F )$ if $F \subset E$.
\end{theorem}

\begin{proof}
If $x \in f^{-1} (E \backslash F )$, then $f(x) \in E \backslash F$. Thus $f(x) \in E$ but $f(x) \notin F$. This means that $x \in f^{-1}(E)$ and but also $x \notin f^{-1}(F)$. In other words, $x \in f^{-1}(E) \backslash f^{-1} (F)$. \\
Assume now that $x \in f^{-1}(E) \backslash f^{-1} (F)$. Then $x \in f^{-1}(E)$ but $x \notin f^{-1}(F)$. This means that $f(x) \in E \backslash F$, and hence $x \in f^{-1} (E \backslash F)$. \\
It follows that  $f^{-1} (E) \backslash f^{-1} (F ) = f^{-1} (E \backslash F)$.
\end{proof}

\begin{theorem}{1.1.11}
If $f : A \to B$ is a function and $E$ and $F$ are subsets of $A$, then $f (E \cup F ) = f (E) \cup f (F )$.
\end{theorem}

\begin{proof}
If $y \in f(E \cup F)$, then $y = f(x)$ for some $x \in E$ or $x \in F$. If $x \in E$, then $y \in f(E)$. If $x \in F$, then $y \in f(F)$. Since $x$ is in either one of these, we know that $y \in f(E) \cup f(F)$. \\
Assume now that $y \in f(E) \cup f(F)$. This implies that $y=f(x)$ for some $x \in E$ or $x\in F$. Thus we can write $x \in E \cup F$. Then $y \in f(E \cup F)$. \\
Since any element of $f(E \cup F)$ is in $f(E) \cup f(F)$ and vice versa, we conclude that $f (E \cup F ) = f (E) \cup f (F )$.
\end{proof}


\begin{theorem}{1.1.12}
If $f : A \to B$ is a function and $E$ and $F$ are subsets of $A$, then $f (E \cap F ) \subset f (E) \cap f (F )$.
\end{theorem}

\begin{proof}
Assume that $y \in f(E \cap F)$. Then $y = f(x)$ for some $x \in E \cap F$. This means that both $x \in E$ as well as $x \in F$. Then, $f(x) \in f(E)$ and $f(x) \in f(F)$, showing that $f(x) \in f(E) \cap f(F)$, or - equivalently - that $y \in f(E) \cap f(F)$. This proves that $f (E \cap F ) \subset f (E) \cap f (F )$.
\end{proof}

\begin{question}{1.1.13}
Give an example of a function $f : A \to B$ and subsets $F\subset E$ of $A$ for
which $f (E) \backslash f (F ) = f (E \backslash F )$. \\

The above conditions are fulfilled for a function $f(x)=x$ with $A=B=[0,10]$, and the subsets $E=[1,6]$ and $F = [1,2] \subset E$.
\end{question}

\begin{exercise}{1.1.14}
\emph{Solved in Class}
\end{exercise}

\begin{exercise}{1.1.15}
\emph{Solved in Class}
\end{exercise}


\section{The Natural Numbers}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Integers and Rational Numbers}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{exercise}{1.3.1}
    Given that $\N$ has an operation of addition which is commutative and associative, how would you define such an addition operation in $\Z$?\\

    Assume $a,b \in \Z$. We differentiate three cases: \\
    a) Both $a,b \in \N$. In this case, proceed as in $\N$.\\
    b) Both $a,b < 0$. In this case $a+b= - (c + d)$ where $c= -a$ and $d= -b$. As such, we may proceed as in the case of natural numbers.\\
    c) assume $a>0$, $b<0$. We now distinguish two cases. Provided $a> -b$, we may reduce $a$ to a sum of two integers $m,n \in \N$ such that $n = -b$. We then define $a+b = m + n + b = m$, since $n+b=0$ by definition of the additive inverse. Assume now instead that $a< -b$. In this case, we can split $-b$ into an addition problem, with two integers $m,n \in \N$ such that $m+n= -b$ and $a = m$. Then
    \[ a+b = a + (-(m+n)) = a + (-m) + (-n) = -n.  \]
    This defines addition in $\Z$.
\end{exercise}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Real Numbers}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{exercise}{1.4.1}
For each of the following sets, describe the set of all upper bounds for the set: \\
(a) the set of odd integers; The integers are unbounded.\\
(b) $\{1 - 1/n : n \in \mathbb{N}\}$; The set of all upper bounds for this set is $\{x \in \mathbb{N}: x \geq 1\}$. \\
(c) $\{r \in \mathbb{Q}: r^3 < 8\}$; The set of all upper bounds for this set is $\{x \in \mathbb{Q}: x \geq 2\}$.\\
(d) $\{ \sin x : x \in \mathbb{R}\}$; The set of all upper bounds for this set is $\{x \in \mathbb{R}: x \geq 1\}$.
\end{exercise}



\begin{exercise}{1.4.2}
For each of the sets in (a), (b), (c) of the preceding exercise, find the least upper bound of the set, if it exists.\\

(a) There is no upper bound, and hence no least upper bound. \\
(b) The least upper bound is $1$.\\
(c) The least upper bound is $2$.
\end{exercise}



\begin{theorem}{1.4.3}
If a subset $A$ of $\mathbb{R}$ is bounded above, then the set of all upper bounds for $A$ is a set of the form $[x, \infty)$. \emph{What is $x$}?
\end{theorem}

\begin{proof}
Let $B$ denote the set of all upper bounds of $A$. By definition, a number $m \in \mathbb{R}$ is considered an upper bound for the set $A$ if $z \leq m$ for all $z \in A$. If the set $A$ has a largest number, then this largest number - $y'$ - will be in the set $B$. In that case, it is obvious that all numbers $m > y'$ will also be upper bounds, since we assumed that $x \leq y'$ for all $x \in A$, and that $m > y'$, it follows that $x \leq y' < m$. Therefore, the set $[y', \infty)$ would be the set of all upper bounds of $A$. \\
Assume now that $A$ does not have a largest number. By the completeness theorem we know that any subset $A$ of an ordered field - such as $\mathbb{R}$ - is indeed bounded above. Specifically, according to theorem 1.4.4 of the book we know that any subset of $\mathbb{R}$ not only is bounded above, but has a least upper bound. By definition, a number $c$ is a least upper bound if and only if it is a number such that $x \leq c$ for all $x \in A$, and for every $k \in \mathbb{R}$, if $k$ is an upper bound of $A$, then $k \geq c$. It is obvious then that the set of all upper bounds of $A$ will be the set $[c, \infty)$ where $c$ is the least upper bound of $A$.
\end{proof}


\begin{exercise}{1.4.4}
If $x^2 < 1-x$, then $x^2 + x - 1 < 0$, hence $$\left( x- \dfrac{-1 + \sqrt{5}}{2} \right) \left( x - \dfrac{-1 - \sqrt{5}}{2} \right) < 0,$$ so $x \in \left( \dfrac{-1 - \sqrt{5}}{2}, \dfrac{-1 + \sqrt{5}}{2} \right),$ hence $A$ is bounded above by $\dfrac{-1 + \sqrt{5}}{2},$ which is its least upper bound.
\end{exercise}

\begin{exercise}{1.4.6}
For the forward direction, observe that for each pair $x, y \in F$ with $x > 0$, $y/x$ in $F$ and since $F$ is archimedean, we have that for some integral $n$, $n > y/x$ so $nx > y$ as desired.

For the reverse direction, we have for each pair $x, y \in F, x>0$, there exists integral $n$ with $nx > y$. In particular, let $x = 1 > 0$, so $n > y$. Hence $F$ is archimedean.
\end{exercise}

\begin{exercise}{1.4.7}
By the archimedean property, there exists natural $n$ with $y-x > 1/n$. Let $k$ be an integer with $k/n < x < (k+1)/n,$ then adding the inequalities $y-x > 1/n$ and $x > k/n$ gives us $y > (k+1)/n$, so $(k+1)/n$ is a rational between $x$ and $y$.

This proves the claim if both $x,y$ are positive. If both are negative, the proof works essentially the same way. If one is positive and the other negative, then take $0$ as the rational number between the two.
\end{exercise}

\begin{exercise}{1.4.8}.
Suppose not, so that $x + r = s \in \mathbb{Q}$ and $xr = s' \in \mathbb{Q}$, then $x = s-r \in \mathbb{Q}$ and $x = s'/r \in \mathbb{Q}$, both of which lead us to a contradiction.
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Sup and Inf}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{exercise}{1.5.1}
For each of the following sets, find the set of all extended real numbers $x$ that are greater than or equal to every element of the set. Then find the sup of the set. Does the set have a maximum? \\
(a) $(-10, 10)$; The set of all numbers greater than this set is the set $[10, +\infty)$. The supremum of the set in question is $10$. The set does not have a maximum.\\
(b) $\{n^2 : n \in \mathbb{N}\}$; In the extended set of real numbers, the only element greater than or equal to all the elements in the set in question is $+\infty$, which thereby must also be its supremum. The set does not have a maximum.\\
(c) $\{ \frac{2n+1}{n+1}\}$; The set of all real numbers greater than the set in question is the set $[2,\infty)$. The supremum is $2$ and the set does not have a maximum.
\end{exercise}



\begin{exercise}{1.5.2}
Find the sup and inf of the following sets. Tell whether each set has a maximum or a minimum. \\
(a) $(-2,8]$; The infimum of the set is $-2$ and the supremum is $8$. The has a maximum, but not a minimum. \\
(b) $\frac{n+2}{n^2+1}$; The infimum of the set is 0, and the supremum is $2$. The set has a maximum, but no minimum.\\
(c) $\{ n/m : n,m \in \mathbb{Z}, n^2<5m^2\}$; The infimum of the set is $- \sqrt{5}$, and the maximum is $\sqrt{5}$. Seeing that $\sqrt{5}$ is not a rational number, the set has neither a maximum nor a minimum.
\end{exercise}




\begin{exercise}{1.5.3}
Prove that if $\sup A < \infty$, then for each $n \in \mathbb{N}$ there is an element $a_n \in A$
such that $\sup A - 1/n < a_n \leq \sup A$.

	\begin{proof}
	This is true since we can easily construct an element $a_n$ such that this equality holds. We assume that $A$ is defined for all $m/n$ with $m,n \in \mathbb{Z}$ within $A$. In this case, we constructs our term to be $a_n = \sup A - 1/(n+1)$. It is obvious that since $1/(n+1) < 1/n$, that $\sup A - 1/n < \sup A - 1/(n+1) \leq \sup A$. \\
	Alternatively, we may also note that $\sup A - 1/n < \sup A$ for all $n \in \mathbb{N}$ by definition, so the inequality holds in the trivial case of $a_n = \sup A$.
	\end{proof}
\end{exercise}




\begin{exercise}{1.5.4}
Prove that if $\sup A = \infty$, then for each $n \in \mathbb{N}$ there is an element $a_n \in A$
such that $a_n > n$.

\begin{proof}
Assume some set $A$ whose supremum is $+\infty$. In that case, $\forall x \in A$, $x < \infty$. Both from the Archimedean property and from the Peano Axioms we know that for every $n \in \mathbb{N}$, there is a successor element $n'$ which is also in $\mathbb{N}$, such that $n < n'$. Since there $\nexists a$ such that $a = \infty$, and $n<\infty$, this implies that $\exists a_n$ such that $a_n = n'$ and $a_n \in A$, showing that $n < a_n < \infty$.
\end{proof}
\end{exercise}




\begin{exercise}{1.5.5}
Formulate and prove the analog of Theorem 1.5.4 for $\inf$. \\

\textbf{Theorem.} Let $A$ be a non-empty subset of $\mathbb{R}$ and let $x$ be an element of $\mathbb{R}$. Then \\
(a) $\inf A \geq x$ if and only if $a \geq x$ for every $a \in A$;\\
(b) $x > \inf A$ if and only if $x > a$ for some some $a \in A$.

	\begin{proof}
	By definition, $a \geq x$ if and only if $x$ is a lower bound for $A$. If $x$ is a lower bound for $A$, then $A$ is bounded below. This implies that its $\inf$ is its greatest lower bound, which is necessarily greater than or equal to $x$. Conversely, if $\inf A \geq x$, then $\inf A$ is finite and is the greatest lower bound for $A$. Since $\inf A \geq x$, $x$ is also  a lower bound for $A$. Thus, $\inf A \geq x$ if and only if $a \geq x$ for every $a \in A$. \\
	If $x > \inf A$, then $x$ is not a lower bound for $A$, which means $x >a$ for some $a \in A$. Conversely, if $x > a$ for some $a  \in A$, then $x> \inf A$, since $a \geq \inf A$. Thus, $x > \inf A$ if and only if $x > a$ for some $a \in A$.
	\end{proof}
\end{exercise}




\begin{exercise}{1.5.6}
Prove part (d) of Theorem 1.5.7.\\

\textbf{Theorem}. Let $A,B$ be non-empty subsets of $\mathbb{R}$. Then $\sup (A-B) = \sup A - \inf B$.

	\begin{proof}
	According to the book, $\sup(A+B) = \sup A + \sup B$ (proof on p. 30). We can then write $\sup (A + (-B) ) = \sup A + \sup(-B)$. We then apply Theorem 1.5.7b, to rewrite $\sup(-B)$ as $-\inf A$. From this it follows that
	\begin{align*}
	\sup(A + (-B)) = \sup(A-B) = \sup A + (- \inf B) = \sup A - \inf B
	\end{align*}.
	\end{proof}
\end{exercise}




\begin{exercise}{1.5.7}
Prove (e) of Theorem 1.5.7. \\

\textbf{Theorem}. Let $A,B$ be non-empty subsets of $\mathbb{R}$. If $A \subset B$, then $\sup A \leq \sup B$ and $\inf B \leq \inf A$.
	\begin{proof}
	If $A \subset B$, then $a \in A$ implies that $a \in B$ for all $a$. Then, if $\sup A \in A$, $\sup A \in B$. Since $\sup B \geq b$ for all $b \in B$, it is obvious that $\sup A \leq \sup B$. Assume now that $\sup A \notin A$. In that case, $\sup A - \epsilon \in A$ for all $\epsilon > 0$. Thus, $\sup A - \epsilon \in A$ and $\sup A - \epsilon \in B$. By definition, $\sup B$ is greater than or equal to all $b \in B$. This means  that if $\sup A - \epsilon \in B$ implies that $\sup A \leq \sup B$. The proof for the infimum is analogous.
	\end{proof}
\end{exercise}

\begin{exercise}{1.5.10}
Prove (a) of Theorem 1.5.10. \\

\textbf{Theorem}. Let $f$ and $g$ be functions defined on a set containing $A$ as a subset, and let $c \in \mathbb{R}$ be a positive constant. Then $\sup_A c f= c \sup_A f$ and $\inf_A c f = c \inf_A f$.
	\begin{proof}
	Let $f$ be function $f: A \to B$. Then $\sup f$ is the supremum of $B$ provided that $f$ is surjective. Let $M$ be an arbitrary upper bound of $cx$ for some $x \in B$. We say that $c x \leq M$ if and only if $x \leq M/c$. This shows that $M$ is an upper bound of $c x$ if and only if $M/c$ is an upper bound of $B$. Hence, $\sup c B = c \sup B$ and similarily $\sup c f = c \sup f$. The result for the infimum follows similarily.
	\end{proof}
\end{exercise}

\begin{exercise}{1.5.8}
\begin{enumerate}[(a)]
\item $\mathrm{sup}_I f = 1, \mathrm{inf}_I f = 0$. These are the max and min respectively.
\item $\mathrm{sup}_I f = \infty, \mathrm{inf}_I f = 3$ at $x = 2$. Neither are the max nor min of $f$ on $I$.
\item $\mathrm{inf}_I f = 0, \mathrm{sup}_I f = 1$. The inf of $f$ on $I$ is actually a minimum, but the sup is not a maximum.
\end{enumerate}


\end{exercise}

\begin{exercise}{1.5.9}
\emph{Solved in Class}
\end{exercise}

\begin{exercise}{1.5.11}
Prove (b) of Theorem 1.5.10. \\

\textbf{Theorem}. Let $f$ and $g$ be functions defined on a set containing $A$ as a subset, and let $c \in \mathbb{R}$ be a positive constant. Then $\sup_A (-f) = - \inf_A f$.

	\begin{proof}
	We have a function $f: A \to B$. A number $x$ is a lower bound for $f(a)$ for all $a \in A$ if and only if $-x$ is an upper bound for the set $-f(a)$. Let  $L$ be the set of all lower bounds for $f(a)$. Then $-L$ is the set of all upper bounds for $-f(a)$. Furthermore, the largest member of $L$ and the smallest member of $-L$ are negatives of each other. That is, $- \inf f(a) = \sup (f(a))$, or equivalently $- \inf f = \sup (-f)$.
	\end{proof}
\end{exercise}

\begin{exercise}{1.5.12}
Prove (c) of Theorem 1.5.10.\\

\textbf{Theorem}. Let $f$ and $g$ be functions defined on a set containing $A$ as a subset, and let $c \in \mathbb{R}$ be a positive constant. Then $\sup_A (f+g) \leq \sup_A f + \sup_A g $ and $\inf_A f + \inf_A g \leq \inf_A (f+g)$.

	\begin{proof}
	By definition, $f(a) \leq \sup f$ for all $a \in A$ and $g(a) \leq \sup g$ for all $a \in A$. Therefore, $f(a)+g(a) \leq \sup f + \sup g$. Let $c$ denote the supremum of $f+g$. We know that $\sup f + \sup g$ is an upper bound  for $f(a)+g(a)$. Since the supremum is always less than or equal to an upper bound, we find that $c \leq \sup f + \sup g$. This implies that $\sup (f+g) \leq \sup f + \sup g$.
	\end{proof}
\end{exercise}

\begin{exercise}{1.5.13}
Prove (d) of Theorem 1.5.10. \\

\textbf{Theorem}. Let $f$ and $g$ be functions defined on a set containing $A$ as a subset, and let $c \in \mathbb{R}$ be a positive constant. Then $\sup \{ f(x) - f(y): x, y \in A \} = \sup_A f - \inf_A f$.
	\begin{proof}
	This appears somewhat obvious. The function $f$ is defined on $A$, i.e., for every $a \in A$, $f$ maps to some value $f(a)$ in some set, let's call it $B$. The value $\sup f$ is defined as to be the least upper bound of $f(a)$, i.e. $\nexists x$ such that $f(x) > \sup f$ for some $x \in A$. The infimum is defined as the value such that there is no value $x \in A$ such that $x < \inf f$. The value defined by $f(x) - f(y)$ for all $x,y \in A$ is a measure of the distance between these two values. Since $\sup f$ and $\inf f$ are defined as above, we can see that there cannot be a greater distance between any other two points in $B$ than the distance between $\sup f$ and $\inf f$. Therefore, for any collection of distances between points in $B$ reached by $f(x)$ for all points $x \in A$, the supremum of this collection - namely, the largest value of this set such that no other value is larger - cannot be any other than the distance between the supremum and the infimum of the function itself.
	\end{proof}
\end{exercise}



\chapter{Sequences}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Limits of Sequences}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{exercise}{2.1.1}
Show that \\
\textbf{(a) if $|x - 5| < 1$, then $x$ is a number greater than $4$ and less than $6$.;}
This is equivalent to saying $-1 < x - 5 < 1$. We add $5$ to the inequality, and we get $4 < x < 6$.\\
\textbf{(b) if $|x - 3| < 1/2$ and $|y - 3| < 1/2$, then $|x - y| < 1$;} We add the inequalities, such that we see $ |x-3| + |y - 3| < 1/2 + 1/2 = 1$. We notice that $|y-3| = |3-y|$. We rewrite using the triangle inequality:
	\begin{align*}
	|(x-3)+(3-y)| \leq |x-3| + |3-y| &< 1 \\
	|x - y| \leq |x-3| + |3-y| &< 1.
	\end{align*}
\textbf{(c) if $|x - a| < 1/2$ and $|y - b| < 1/2$, then $|x + y - (a + b)| < 1$.} We add the inequalities and get $|x-a| + |y-b| < 1/2 + 1/2 =1$. We can then rewrite using the triangle inequality as above
	\begin{align*}
		|(x-a) + (y-b) | &\leq |x-a| + |y-b| & < 1 \\
		|x+y - a-b| &\leq |x-a|+ |y-b| &<1\\
		|x+y - (a-b)|  &\leq |x-a|+ |y-b| &<1.
	\end{align*}
\end{exercise}

\begin{exercise}{2.1.2}
Adding the two inequalities gives $$|x-1| + |x-2| < 1,$$ hence by the triangle inequality $|2x - 3| \leq |x-1| + |x-2| < 1,$ so $|x -\frac{3}{2}|<\frac{1}{2}$. Hence $x \in (1,2)$. Combining this with $|x-1| < 1/2$ and $|x-2|< 1/2$ we have $x \in (1,\frac{3}{2})$ and $x \in (\frac{3}{2}, 2)$, a contradiction.
\end{exercise}

\begin{exercise}{2.1.3}
Put each of the following sequences in the form $a_1, a_2, a_3, \hdots, a_n$. This requires that you compute the first 3 terms and find an expression for the $n$th term. \\
\textbf{(a) the sequence of positive odd integers;} This is a sequence of the form $1,3,5,\hdots$. To find the $n-th$ term, we express this sequence as $a_n = 2 n - 1$, with $n \in \N$. \\
\textbf{(b) the sequence defined inductively by $a_1 = 1$ and $a_{n+1} = - \frac{a_n}{2}$;} The sequence begins with $1, -1/2, 1/4, \hdots$. The $n$th term will be something like $a_n= ((-1)^{n-1})/(2^{n-1})$ for $n \in \N$. \\
\textbf{(c) the sequence defined inductively by $a_1 = 1$ and $a_{n+1} = \frac{a_n}{n+1}$}. This is the series $1, 1/3, 1/12, 1/60, \hdots$. The $n$th term is: $a_n = \frac{2}{(n+1)!}$.
\end{exercise}

\begin{exercise}{2.1.4}
Find $\lim 1/n^2$. \\

The larger $n$ become, the smaller $1/n^2$ will become. We guess the limit to be $0$. For any $\epsilon > 0$, we need an $N$ such that whenever $n > N$, $1/n^2 < \epsilon$. We find that this is true whenever $ 1/\epsilon < n^2$, or in other words - whenever $\sqrt{1/ \epsilon} < n$.
\end{exercise}


\begin{exercise}{2.1.5}
Find $\lim  \frac{2n-1}{3n+1}$. \\

We guess the limit to be $2/3$.
	\begin{align*}
	|\frac{2n-1}{3n+1} - \frac{2}{3}| = | \frac{3(2n-1) - 2(3n+1)}{3(3n+1)}| = | \frac{6n-3-6n-2}{9n+3} |\\
	= |\frac{5}{9n+3} < | \frac{5}{9n} | < | \frac{5}{n} |
	\end{align*}
We must choose an $n > N$ such that $N> \frac{5}{\epsilon}$ so that this will be true.
\end{exercise}


\begin{exercise}{2.1.6}
Find $\lim (-1)^n /n$ \\

We guess the limit to be $0$. We see $| \frac{(-1)^n}{n}| = |\frac{1}{n}|$. Hence we need to choose an $n>N$ such that $N> \frac{1}{\epsilon}$ for this inequality to be true.
\end{exercise}

\begin{exercise}{2.1.7}
The limit is zero. Let $N = \dfrac{1}{\sqrt{\epsilon}}.$ Then if $n > N$, $$\left| \dfrac{n}{n^3 + 4} - 0 \right| = \dfrac{n}{n^3 + 4} < \dfrac{n}{n^3} = \dfrac{1}{n^2} < \dfrac{1}{N^2} = \epsilon.$$ Hence $\displaystyle \lim_{n \rightarrow \infty} \dfrac{n}{n^3+4} = 0.$
\end{exercise}

\begin{exercise}{2.1.8}
The limit is zero. Let $N = \left|\dfrac{1-\epsilon^2}{2\epsilon} \right|^2.$ Then for $n > N$, $$\sqrt{n} > \left| \dfrac{1-\epsilon^2}{2\epsilon} \right| \Rightarrow 2\epsilon \sqrt{n} > |1-\epsilon^2|,$$ hence adding $\epsilon^2 + n$ to both sides yields $$\epsilon^2 + n + 2\epsilon \sqrt{n} > |1-\epsilon^2| + \epsilon^2 + n \geq 1 + n,$$ where the last step follows from the triangle inequality. Factoring the left side gives $$(\sqrt{n} + \epsilon)^2 > n+1,$$ and since both sides are positive we may take the square root which yields $$\sqrt{n} + \epsilon > \sqrt{n+1} \Rightarrow \epsilon > \sqrt{n+1} - \sqrt{n} = |\sqrt{n+1} - \sqrt{n} - 0|,$$ hence $\displaystyle \lim_{n \rightarrow \infty} \sqrt{n+1} - \sqrt{n} = 0.$
\end{exercise}

\begin{exercise}{2.1.9}
The limit is zero. We know for any $\epsilon > 0,$ there exists $N_1$ such that for all $n > N$, $$\dfrac{1}{n} < \epsilon/2,$$ since $\epsilon/2$ is another positive real and we have shown the limit of $1/n$ as $n$ goes to infinity is zero in problem 4. Similarly, we may find $N_2$ such that for all $n > N_2,$ $$\left|\dfrac{(-1)^n}{n^2}\right| < \epsilon/2,$$ since we found the limit as $n \rightarrow \infty$ of $(-1)^n/n$ earlier to be $0$, and it can be shown in much the same way that $\displaystyle \lim_{n \rightarrow \infty} (-1)^n/n^2 = 0.$ Hence, for $n > \max \{N_1, N_2\},$ both inequalities hold and we may add the two and apply the triangle inequalityto obtain $$\left|\dfrac{1}{n} + \dfrac{(-1)^n}{n^2} - 0\right| \leq \left|\dfrac{1}{n}\right| + \left|\dfrac{(-1)^n}{n^2}\right| < \epsilon,$$ hence the limit is zero.
\end{exercise}


\begin{exercise}{2.1.10}
Prove that $\lim 2^{-n} = 0$. Hint: prove first that $2^n \geq n$ for all natural numbers $n$.\\

	\begin{proof}
	We wish to show that $2^n > n$ for all $n$. Proof by induction. The base case, $2^1 > 1$ is obviously true, since $2^1 = 2$. We assume now that $2^n > n$ for some $n$. Then we wish to check $2^{n+1}$. But, we can rewrite this simply as $2^n 2^1$. Let $k= 2^n$. Since we know that $k > n$, it is obvious that $2 k > n+1$. Thus, $2^n > n$ for all $n \in \N$. \\
	We note that $2^{-n} = \frac{1}{2^n}$. Thus, $\lim 2^{-n}  = \lim \frac{1}{2^n}$. Since $2^n$ increases until infinity, we see that $1/2^{n}$ will grow smaller and smaller, since $1/2^{n} > 1/2^{n+1}$ for all $n$.\\
	We see that for any $\epsilon > 0$, we need to simply pick $n$ such that $1/\epsilon < 2^n$.  As such, the limit is 0.
	\end{proof}
\end{exercise}



\begin{exercise}{2.1.11}
Prove that if $a_n \to 0$ and $k$ is any constant, then $k a_n \to 0$. \\

If $a_n \to 0$, this means that $a_n < \epsilon$ for any $\epsilon > 0$. We multiply by $k$ and find that $k a_n < k \epsilon$.
\end{exercise}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Using the Definition of Limit}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{exercise}{2.2.1}
Make an educated guess as to what you think the limit is, then use the definition of limit to prove that your guess is correct.\\

$\lim \frac{3n^2 -2}{n^2 +1}$. I assume the limit will be $3$. We note that $\frac{3n^2 - 2}{n^2+1} <  \frac{3n^2}{n^2} = 3$. Hence, the limit is 3.
\end{exercise}

\begin{exercise}{2.2.2}
The limit is zero. Let $N = \dfrac{1}{\epsilon}$, then $$\left| \dfrac{n}{n^2 + 2} - 0 \right| = \dfrac{n}{n^2+2} < \dfrac{n}{n^2} = \dfrac{1}{n} < \dfrac{1}{N} = \epsilon.$$
\end{exercise}

\begin{exercise}{2.2.3}
Make an educated guess as to what you think the limit is, then use the definition of limit to prove that your guess is correct.\\

$\lim \frac{1}{\sqrt{n}} $ I assume the limit will be $0$. We see $| \frac{1}{\sqrt{n}}| = | \frac{1}{n^{1/2}}|$; therefore, this is true whenever we choose an $n>N$ such that $\sqrt{N}> \frac{1}{\epsilon}$.
\end{exercise}

\begin{exercise}{2.2.4}
The limit is 1. Let $N = \dfrac{2}{\epsilon}$, then $$\left| \left(\dfrac{n}{n+1}\right)^2 - 1 \right| = \left| \dfrac{-2n-1}{n^2 + 2n + 1} \right| = \dfrac{2n+1}{n^2 + 2n + 1}< \dfrac{2}{n}.$$ This last step is true because for any positive integer $n$, we have $3n+2 > 0,$ hence $2n^2 + 4n + 2 > 2n^2 + n$, and so dividing by $n(n^2 + 2n + 1)$ on both sides yields the desired inequality. Hence, $$\left| \left(\dfrac{n}{n+1}\right)^2 - 1 \right| < \dfrac{2}{n} < \dfrac{2}{N} = \epsilon.$$
\end{exercise}


\begin{exercise}{2.2.5}
Make an educated guess as to what you think the limit is, then use the definition of limit to prove that your guess is correct.\\

The limit is $\dfrac{1}{2}$. Let $A = \{ n \in \mathbb{N} : |a_n - \dfrac{1}{2}| \geq \epsilon\}$ for a given $\epsilon.$ This set is finite since \begin{align*}
\sqrt{n^2 + n} - n - \dfrac{1}{2} \geq \epsilon & \Leftrightarrow n^2 + n \geq n^2 + \left(\epsilon + \dfrac{1}{2}\right)^2 + n(2\epsilon + 1) \\
& \Leftrightarrow 0 \geq \left( \epsilon + \dfrac{1}{2} \right)^2 + 2 \epsilon n. \end{align*}
But this last inequality is never true, since each of the terms is strictly greater than zero. Of course, we might also have
\begin{align*}
n+\dfrac{1}{2} - \sqrt{n^2 + n} \geq \epsilon & \Leftrightarrow n^2 + +n(1-2\epsilon) + \epsilon^2 \geq n^2 + n \\
& \Leftrightarrow -2n\epsilon + \epsilon^2 \geq 0 \\
& \Leftrightarrow \dfrac{\epsilon}{2} \geq n,
\end{align*}
which is certainly only true for finite $n$. Hence for a given $\epsilon$ the set $A$ is in fact finite, so the limit is $\dfrac{1}{2}.$
\end{exercise}


\begin{exercise}{2.2.6}
Make an educated guess as to what you think the limit is, then use the definition of limit to prove that your guess is correct.\\

$\lim (1 + 1/n)^3 = 1$. Proof:
	\begin{align*}
		| (1+ \frac{1}{n})^3 -1 | &= | \frac{1}{n^3} + \frac{3}{n^2}+ \frac{3}{n} + 1 - 1| = | \frac{1}{n^3} + \frac{3}{n^2}+ \frac{3}{n} |
		\end{align*}
We note that each term is of the form $c/n$ or multiples thereof for some constant $c$. It has already been shown that each such term tends can be made smaller than any $\epsilon$. This also holds for the sum.
\end{exercise}

\begin{exercise}{2.2.7}
Suppose the sequence converges to $a$, then for all $\epsilon > 0,$ there is an $N$ so that when $n > N$, $|a_n - a| < \epsilon \Rightarrow a- \epsilon < a_n < a+\epsilon.$ In particular, let $\epsilon = 1$, then certainly for all $n > N$, the sequence is bounded above by $a+1$ and below by $a-1$. For $n \leq < N,$ notice that the $a_1, ..., a_N$ is bounded above by $e = \max \{|a_1|, ..., |a_n|\}$ and below by $f = \min \{a_1, ..., a_N\}.$ Hence the entire sequence is bounded above by $\max \{e, a+1\}$ and $\min \{f, a-1\}.$ $\square$
\end{exercise}


\begin{exercise}{2.2.8}
Prove that if $\lim a_n = a$, then $\lim a_n^3 = a^3$.
	\begin{align*}
	|a_n^3 - a^3 | = |(a_n - a) (a_n^2 + a_n a + a^2)|
	\end{align*}
We then note that we are given that $|a_n - a| < \epsilon$. From this we see that
	\begin{align*}
	|(a_n - a) (a_n^2 + a_n a + a^2)| < \epsilon (a_n^2 + a_n a + a^2).
	\end{align*}
\end{exercise}


\begin{exercise}{2.2.9}
Does the sequence $\{cos(n \pi /3)\}$ have a limit? Justify your answer. \\

No. The sequence $\{cos(n \pi /3)\}$ oscillates between $-1$ and $1$; a limit cannot converge to two different values. Hence, this sequence does not have a limit.
\end{exercise}

\begin{exercise}{2.2.10}
For a similar reason as in the previous problem, the sequence $a_n = \cos (\pi n)$ does not converge but the sequence $|a_n| = |\cos (\pi n)|$ does converge since $|a_n| = 1$ for all $n$.
\end{exercise}

\begin{exercise}{2.2.11}
Prove that if $\{a_n\}$ and $\{b_n\}$ are sequences with $|a_n| \leq b_n$ for all $n$ and if $\lim b_n =0$, then $\lim a_n = 0$ also. \\

We are given that $ |a_n|  \leq b_n$ for all $n$. Therefore, we know that $ \lim |a_n| \leq \lim b_n$. We know that $\lim b_n = 0$. Hence we can write - equivalently - that $\lim |a_n| \leq 0$. We notice that $|a_n|$ is defined to be greater than or equal to zero. Hence we have $0 \leq \lim |a_n| \leq 0$, from which it follows by the squeeze theorem (proof on p. 43 of the book) $\lim |a_n| = 0$.
\end{exercise}


\begin{exercise}{2.2.12}
Prove the following partial converse to Theorem 2.2.3: Suppose $\{a_n\}$ is a convergent sequence. If there is an $N$ such that $a_n \leq c$ for all $n > N$, then $\lim a_n \leq c$. Also, if there is an $N$ such that $b \leq a_n$ for all $n > N$, then $b \leq \lim a_n$. \\

Note that ${a_n}$ is bounded by $c$ according to the premise. In this case, we can say that $a_n \leq \sup a_n \leq c$ for all $n$. Let $ a = \lim a_n$. We know by definition that $a \leq \sup a_n$, and therefore we can write that $\lim a_n \leq \sup a_n \leq c$. \\
Likewise, we can say that $b$ is a lower bound for $a_n$ such that $b \leq \inf a_n$. We know that by definition $\inf a_n \leq a$, allowing us to write $b \leq \lim a_n$.
\end{exercise}

\begin{exercise}{2.2.15}
Suppose this sequence converges to zero. Then for all $\epsilon > 0$, there exists $N$ such that for all $n > N$, $|a_n| < \epsilon.$ But consider $n = \displaystyle \left(\left \lfloor \dfrac{N}{10^6} \right \rfloor + 1\right ) \cdot 10^6.$ This is a multiple of one million, so $a_n \geq \epsilon.$ And, it is larger than $N$ since $$\dfrac{N}{10^6} < \left \lfloor \dfrac{N}{10^6} \right \rfloor+1$$ by definition, $10^6 \left(\left \lfloor \dfrac{N}{10^6} \right \rfloor + 1\right) > N.$ Hence we have a contradiction.
\end{exercise}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Limit Theorems}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{exercise}{2.3.1}
\emph{Solved in Class}
\end{exercise}


\begin{exercise}{2.3.2}
Use the Main Limit Theorem to find $\lim \frac{n^2 - 5}{n^3 + 2n^2 + 5}$. \\
	\begin{align*}
		\lim \frac{n^2 - 5}{n^3 + 2n^2 + 5} &=  &(\text{dividing top and bottom by $n^3$}) \\
		\lim \frac{1/n - 5/n^3}{1+2/n + 5/n^3} &= \\
		\frac{\lim (1/n - 5/n^3)}{\lim (1+2/n + 5/n^3)} &= \frac{0}{1} = 0.
	\end{align*}
\end{exercise}

\begin{exercise}{2.3.3}
By dividing numerator and denominator by $2^n$, in a similar fashion to problem 1, we can see that the limit is 1.



\end{exercise}

\begin{exercise}{2.3.4}
Let $a_n = 1/n$, then $\lim a_n = 0,$ and let $b_n = \sin n$, then $|b_n| \leq 1$. By theorem 2.3.2, the sequence $\dfrac{\sin n}{n} = a_n b_n$ has limit zero.
\end{exercise}


\begin{exercise}{2.3.5}
Prove Theorem 2.3.2.

	\begin{proof}
	We know that $\lim a_n = 0$, hence we know that for all $\epsilon > 0$, there exists an $N$ such that whenever $n>N$, $|a_n | < \epsilon$. Likewise, we know that $b_n$ is bounded, such that we can state that $- q \leq b_b \leq q$. We can then also write $|a_n| < \frac{\epsilon}{|q|}$.  We guess that the limit of $ (a_n)(b_n)$ is zero, so we write:
		\begin{align*}
		|a_n b_n - 0| = |a_n b_n| &\leq | a_n q| \\
		|a_n b_n| &\leq |q| \frac{\epsilon}{|q|}  \\
		|a_n b_n| &\leq \epsilon.
		\end{align*}
	Thus, the $\lim a_n b_n = 0$, since we there is an $N$ such that the above inequality is true whenever we pick an $n>N$.
	\end{proof}
\end{exercise}


\begin{exercise}{2.3.6}
Prove that a sequence $\{a_n\}$ is both bounded above and bounded below if and only if its sequence of absolute values $\{|a_n|\}$ is bounded above.

\begin{proof}
By definition, if $\{|a_n|\}$ is bounded above, then there exists some $M$ such that $|a_n| \leq M$ for all $n$. This is equivalent to saying $-M \leq a_n \leq M$, which proves that $\{a_n\}$ is bounded above and below.
\end{proof}
\end{exercise}


\begin{exercise}{2.3.7}
Prove part(b) of Theorem 2.3.6.

\begin{proof}
	Since both $a_n$ and $b_n$ have a limit, we can write $|a_n -a| < \frac{\epsilon}{2}$ and $|b_n - b| < \frac{\epsilon}{2}$. For all $\epsilon$, we have an $N$ such that if we choose $n>N$, these inequalities are true. We know add them together and find
	\begin{align*}
	|a_n - a| + |b_n - b| &< \epsilon \\
	|(a_n) + (b_n -b) | \leq |a_n - a| + |b_n - b| &< \epsilon \\
	|(a_n + b_n) - (a+b) | \leq |a_n - a| + |b_n - b| &< \epsilon.
	\end{align*}
\end{proof}
\end{exercise}


\begin{exercise}{2.3.8}
Prove that if $\{b_n\}$ is a sequence of positive terms and $b_n \to b > 0$, then there is a number $m>0$ such that $b_n \geq m$ for all $n$. \\

This is true by virtue of the definition of $\mathbb{R}$. The statement above is equivalent to saying that we are looking for some $m$ such that $0 < m \leq b_n$. By definition $\mathbb{R}$ is full, such that between any two numbers, there are infinitely more numbers.
\end{exercise}


\begin{exercise}{2.3.9}
Prove part (d) of Theorem 2.3.6. Hint: Use the previous exercise. I.e, that if $a_n \to a$ and $b_n \to b$, $a_n/b_n \to a/b$, if $b \neq 0$ and $b_n \neq 0$ for all $n$.

	\begin{proof}
		\begin{align*}
		|a_n \frac{1}{b_n} - a \frac{1}{b}| = |a_n \frac{1}{b_n} - a \frac{1}{b_n} + a \frac{1}{b_n} - a \frac{1}{b} | \leq |a_n = a| |\frac{1}{b_n}| + |a| |\frac{1}{b_n -b}|
		\end{align*}
		We know that $\{1/b_n\}$ is bounded, and hence $\{|1/b_n|\}$ is bounded above. We also have $|a_n -a| \to 0$. Therefore, $|a_n -a| |1/b_n| \to 0$. Also, $ |a| |1/b_n - 1/b| \to 0$. By (b) we know that $ |a_n - a| |1/b_n| + |a| |1/b_n - 1/b| \to 0$, proving that $a_n/b_n \to a/b$.
	\end{proof}
\end{exercise}


\begin{exercise}{2.3.10}
Prove part (f) of theorem 2.3.6. Hint: use the identity:
	\begin{align*}
	x^k - y^k = (x-y)(x^{k-1} + x^{k-2} y+ \hdots  + y^{k-1})
	\end{align*}
with $x= a_n^{1/k}$ and $y= a^{1/k}$, to show that $a_n^{1/k} \to a^{1/k}$ if $a_n \geq 0$ for all $n$.

	\begin{proof}
		We notice that
		\begin{align*}
			a_n^{1/k} - a^{1/K} = (a_n - a) (a_n^{1/K-1} + a_n^{1/K -2}a^{1/k} + \hdots + a^{1/K-1}) = (a_n - a) b_n
		\end{align*}
		where
		\[ b_n = a_n^{1/K-1} + a_n^{1/K -2}a^{1/k} + \hdots + a^{1/K-1} \]
		We know that $\{a_n\}$ converges, and hence that $\{|a_n|\}$ is bounded above. We choose an upper bound $m$ for $\{|a_n|\}$ which satisfies that $|a_n| \leq m$. Then $b_n \leq \frac{1/k} m^{1/k}$ sowing that $\{|b_n|\}$ is bounded above. According to theorem 2.3.2 we conclude that $|a_n - a| |b_n| \to -$ and find from theorem 2.3.1 that $a_n^{1/k} \to a^{1/k}$.
	\end{proof}
\end{exercise}


\begin{exercise}{2.3.12}
Prove that $\lim a^{1/n} = 1$. Hint: use the result of the previous exercise.\\

We notice that $n^{1/n} > 1$ for all $n \in \N$.We can therefore write that we are looking for a solution to $n^{1/n} - 1 < \epsilon$.We can rearrange and raise both sides to the $n$th power, resulting in the equation $n < (1 + \epsilon)^n$. We can expand the right hand side using the binomial theorem:
	\begin{align*}
	n < 1+ n \epsilon + \frac{1}{2}n(n-1) \epsilon^2 + \hdots
	\end{align*}
As long as $n < \frac{1}{2} n(n-1) \epsilon^2$ this inequality holds, requiring that $n> 1+ \frac{2}{\epsilon^2}$. Therefore, for any $\epsilon >0$ there exists an $N$ such that whenever $n>N$,  $| n^{1/n} -1|< \epsilon$.
\end{exercise}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Monotone Sequences}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{exercise}{2.4.1}
Tell which of the following sequences are non-increasing, non-decreasing, bounded? Justify your answers.\\
(a) $\{n^2\}$; for $n \in \N$, this sequence is non-decreasing since $n^2 < (n+1)^2$ for all $n$. It is bounded below by $1$.\\
(b) $\{ \frac{1}{\sqrt{n}} \}$; this sequence is non-increasing, since $\sqrt{n}=n^{1/2}<(n+1)^{1/2}$ for all $n$. This then implies $\frac{1}{\sqrt{n}} > \frac{1}{\sqrt{n+1}}$. The sequence is bounded by $0$ and $1$. \\
(c) $\{ \frac{(-1)^n}{n} \}$;  this sequence is neither non-increasing, nor non-decreasing as the sign of the value of the sequence fluctuates due to the term $(-1)^n$. It is, however, bounded by $-1$ and $1/2$.\\
(d) $\{ \frac{n}{2^n} \}$; this is the sequence $\frac{1}{2},\frac{2}{4},\frac{3}{8},\hdots$ which is clearly non-increasing. It is bounded by $0$ and $1$. \\
(e) $\{ \frac{n}{n+1} \}$; this is the sequence $\frac{1}{2}, \frac{2}{3},\frac{3}{4},\frac{4}{5},\hdots$ which is clearly non-decreasing and tending to $1$. It is bounded by $1/2$ and $1$.
\end{exercise}


\begin{exercise}{2.4.2}
	Prove that the sequence ${x_n}$ with $x_1=1$ and $x_{n+1} = \sqrt{x_n + 1}$ converges and decide what number it converges to.
	\begin{proof}
		The first few terms of the sequence: $1, \sqrt{2}, \sqrt{1+\sqrt{2}}, \sqrt{1+\sqrt{1+\sqrt{2}}}, \hdots$. We see that $x_n$ is both increasing an bounded:
			\[ x_{n+1} = \underbrace{\sqrt{1+\sqrt{1+ \hdots \sqrt{1+\sqrt{2}}}}}_\text{$n$ terms} > \underbrace{\sqrt{1+\sqrt{1+ \hdots \sqrt{1+\sqrt{2}}}}}_\text{$n-1$ terms} = x_n \]
			Then $x_n$ is increasing for all $n \in \N$. We prove that $x_n < 2$ by induction. We have $x_1=1<2$. Then, if $x_k<2$, $x_{k+1} = \sqrt{1+x_k}< \sqrt{1+2}= \sqrt{3}<2$. Thus by the monotone convergence theorem, $x_n$ converges and is bounded by 2. \\
		Finding the limit: We solve $a=\sqrt{1+a}$, which implies $a^2 - a -1 =0$. The solutions to this are $a_1 = \frac{1+\sqrt{5}}{2}$, $a_2= \frac{1-\sqrt{5}}{2}$. The correct limit is $a_1$, since $a_2 < 0$.
	\end{proof}
\end{exercise}

\begin{exercise}{2.4.3}
If $a_1 = 1$ and $a_{n+1} = (1-2^{-n})a_n$ , prove that $\{a_n\}$ converges.
	\begin{proof}
We notice that $2^{-n}$ is monotone and converges to $0$. Therefore we see that $1-2^{-n}$ is also monotone, converging to $1$. The whole term then is monotone and non-increasing. It is also bounded by 0 and 1. Therefore, by the monotone convergence theorem, $a_n$ converges.
	\end{proof}
\end{exercise}


\begin{exercise}{2.4.4}

\end{exercise}


\begin{exercise}{2.4.8}
Prove that $\lim \frac{n^5 + 3n^3 + 2}{n^4 -n+1}= \infty$.
	\begin{align*}
	\lim \frac{n^5 + 3n^3 + 2}{n^4 -n+1} &= \lim \frac{n^5(1+3n^3/n^5+2/n^5)}{n^4(1-n/n^4+1/n^4)}\\
	&= \lim \underbrace{n}_\text{$\to \infty$} (\underbrace{\frac{1+3/n^2+2/n^5}{1-1/n^3+1/n^4}}_\text{$\to 1$})
	\end{align*}
And hence, $\lim \frac{n^5 + 3n^3 + 2}{n^4 -n+1}= \infty$.
\end{exercise}

\begin{exercise}{2.4.10}
If the sequence is bounded, then we are done by theorem 2.4.1. Otherwise there is no $M \in \mathbb{R}$ such that $M \geq a_n$ for all $n$. This is equivalent to saying any $M \in \mathbb{R}$, there exists $N$ such that $a_N > M$. Since the sequence is monotone, for all $n > N$, $a_n > a_N > M$. Hence it has limit equal to infinity.
\end{exercise}

\begin{exercise}{2.4.11}
Prove Part (c) of Theorem 2.4.7. \\
 	\textbf{Theorem} $\lim a_n = \infty$ iff $\lim(-a_n)= -\infty $

 	\begin{proof}
 	If $a_n \to \infty$, there exists some value of $a_n$ such that $a_n > M$ for any possible $M\in\R$. If we consider the sequence $a_n (-1)$, we see clearly that $-a_n < M$ for any $M\in\R$. But if that is true, then $\lim -a_n= -\infty$.
 	\end{proof}
\end{exercise}

\begin{exercise}{2.4.12}
Suppose $\lim b_n < \infty$. Then $b_n$ is bounded by theorem 2.2.3, so there exists $M$ with $b_n \leq M$ for all $n$. Since $a_n \leq b_n,$ we have $a_n \leq M$ so $\lim a_n < \infty$, a contradiction.
\end{exercise}

\begin{exercise}{2.4.13}
For all $M \in \mathbb{R}$, given $k \geq 0$, $M/k \in \mathbb{R}$. Hence there exists $N$, for all $n > N$, $a_n > M/k$. Since $b_n \geq k$, we have $a_n b_n > k \cdot M/k = M,$ hence $\lim a_n b_n = \infty$.
\end{exercise}

\begin{exercise}{2.4.14}
\emph{Solved in Class}
\end{exercise}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Cauchy Sequences}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{exercise}{2.5.1}
Give an example of a nested sequence of bounded open intervals that does not have a point in its intersection.
\end{exercise}

\begin{exercise}{2.5.2}
Let $I_n = [n, \infty)$ which are nested, closed and unbounded. Suppose there is an $x \in \mathbb{R}$ with $x \in \displaystyle \bigcap_i I_i$. But there exists an integer $m, m>x$ by the archimedean property. Hence $I_m$ does not contain $x$, a contradiction.
\end{exercise}

\begin{exercise}{2.5.4}
Prove by induction that if $\{n_k\}$ is an increasing sequence of natural numbers, then $n_k \geq k$ for all $k$.
	\begin{proof}
	Assume the base case $n_k=n$, which is the series $1,2,3,4,5,\hdots$. Since $k$ is the counter index, i.e. $k \in \N$, it is obvious that $n_k = k = 1,2,3,4,5, \hdots$. We generalize to the $n+1$ case, i.e. $n_k = n+1$. In that case we have $n_k = n+1 = k+1 > k$.
	\end{proof}
\end{exercise}

\begin{exercise}{2.5.5}
Which of the following sequences $\{a_n \}$ have a convergent subsequence? Justify your answer. \\
(a) $a_n= (-2)^n$; None of the subsequences are convergent, as they either tend to $+\infty$ or $-\infty$.\\
(b) $a_n = \frac{5+(-1)^n n}{2+3n}$; This sequence is convergent for all $n$ such that $n\mod 2=0$, which is the sequence starting with $0.875, 0.6428, 0.55, 0.5, 0.46875, 0.4473, 0.4318, 0.42, 0.41071, \hdots$\\
(c) $a_n=2^{(-1)^n}$ This sequence has convergent subsequences for all $n$ such that $n\mod 2=0$ and for $n \mod 2 = 1$.
\end{exercise}

\begin{exercise}{2.5.6}
\begin{enumerate}[(a)]
\item The sequence is $-1, 1, -1, 1, -1, 1, ...$. The subsequence $\{a_{n_i}\}, n_i = 2i-1$ converges to $-1$.
\item The sequence is $\sqrt{2}/2, 1, \sqrt{2}/2, - \sqrt{2}/2, -1, - \sqrt{2}/2, 0, ...$. Since the sequence is periodic, we can find infinitely many terms equal to $0$, which gives a convergent subsequence (convergent to 0).
\item Let $a_{n_k} = 1/2^k$ which happens when $n = 2^k + 1$. Then by a previous homework this has limit 0. Hence it is a convergent subsequence.
\end{enumerate}
\end{exercise}

\begin{exercise}{2.5.7}
For each of the following sequences, determine how many different limits of subsequences there are. Justify your answer.\\
(a) $\{1+(-1)^n\}$; This sequence is $0,2,0,2,0,2,\hdots$ and as such has two different limits: 0 and 2. \\
(b) $\{cos(n \pi/3)\}$; There are two different limits. The first approaches $1$ for the sequence of all $n$ where $n\mod 6=0$. The second limit is attained at $-1$ for all $n$ such that $n\mod 6=3$. \\
(c) $1, \frac{1}{2},1,\frac{1}{2}, \frac{1}{3},1, \frac{1}{2},\frac{1}{3},\frac{1}{4},1. \frac{1}{2},\frac{1}{3},\frac{1}{4}, \frac{1}{5},\hdots$ The terms $a_1, a_3, a_6, a_{10}, a_15, \hdots$ are convergent to 1.
\end{exercise}

\begin{exercise}{2.5.8}
Does the sequence $\sin n$ have a convergent subsequence? Why? \\

Yes, it has three convergent subsequences, provided $n\in \R$. If $n\in \N$, then it is not convergent.
\end{exercise}

\begin{exercise}{2.5.9}
Prove that a sequence which satisfies $|a_{n+1} - a_n| < 2^{-n}$ for all $n$ is a Cauchy sequence.

	\begin{proof}
	We notice that the sequences defined by the above condition are non-increasing and covergent. We notice the following pattern:
		\begin{align*}
		|a_{n+2}-a_n| = |a_{n+2}-a_{n+1}+a_{n+1}-a_n| &\leq |a_{n+2} - a_{n+1}|+ |a_{n+1} - a_n| < 2^{-n+1} + 2^{-n} \\
		|a_{n+3}-a_n| = |a_{n+3}-a_{n+2}+a_{n+2}-a_{n+1}+a_{n+1}-a_n| &\leq |a_{n+3}-a_{n+2}|+|a_{n+2}-a_{n+1}|+|a_{n+1}-a_n|\\
		&< 2^{-(n+2)} + 2^{-(n+1)} + 2^{-n}
		\end{align*}
	Inductively, we see that this pattern continues for all patterns $a_n$ and $a_n+k$ with $k \in \N$. Now, we assume two indices $m$ and $n$ such that $m>n$. We find
		\begin{align*}
		|a_m - a_n| &< 2^{-(m-1)} + 2^{-(m-2)}+\hdots+2^{-n} = \\
				& 2^{-n} \underbrace{ (1+ 2^{-1}+2^{-2}+\hdots+2^{-(m-1)+n})}_\text{geometric series}
		\end{align*}
	We rewrite and solve the geometric series:
		\begin{align*}
		2^{-n}( \sum_{k=-m+n+1}^0 2^k)= 2^{-n}(2-2^{-m+n+1}) = 2^{1-n} - 2^{1-m}
		\end{align*}
	We want to to prove $|2^{1-n} - 2^{1-m}| \leq 2^{1-n} + 2^{1-m} < \epsilon$. We solve the equations $2^{1-n} < \frac{\epsilon}{2}$ and $2^{1-m} < \frac{\epsilon}{2}$. The solution to this is $2- \frac{\log(\epsilon)}{\log(2)}< n,m$
	\end{proof}
\end{exercise}


\begin{exercise}{2.5.10}
\emph{Solved in class}
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{lim inf and lim sup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{exercise}{2.6.1}
\emph{Solved in Class}
\end{exercise}

\begin{exercise}{2.6.2}
Find lim inf and lim sup for the sequence $a_n = \frac{n}{2^{k_n}}-1$ with $k_n$ being the largest integer $k$ so that $2^k \leq n$. \\

This is the sequence $0, 0, \frac{1}{2},0, \frac{1}{4}, \frac{2}{4}, \frac{3}{4}, 0, \frac{1}{8}, \frac{2}{8}, \frac{3}{8}, \frac{4}{8}, \frac{5}{8}, \frac{6}{8}, \frac{7}{8}, 0, \hdots $. It is clear that $\lim \inf =0$ and $\lim \sup = 1$.
\end{exercise}


\begin{exercise}{2.6.3}
Find lim inf and lim sup for the sequence $1, \frac{1}{2}, 1,\frac{1}{2},\frac{1}{3},1,\frac{1}{2},\frac{1}{3}, \frac{1}{4},1,\hdots$. \\

We find that $\lim \inf = 0$ and $\lim \sup =1$.
\end{exercise}


\begin{exercise}{2.6.4}
\emph{Solved in class}
\end{exercise}

\begin{exercise}{2.6.5}
If $\lim \sup a_n$ is finite, prove that $\lim \inf(-a_n) = - \lim \sup a_n$.

	\begin{proof}
	By assumption, $\lim \sup a_n$ is equal to some $a$, such that $a \geq a_n$ for all $a_n \in \{a_n\}$. We multiply this by $-1$ to find the inverse sequence $\{-a_n\}$. Then we have $-a \leq -a_n$ for all $a_n \in \{a_n\}$. By definition, this means $-a = \lim \inf (-a_n)$. Therefore, $\lim \inf (-a_n) = - \lim \sup a_n$.
	\end{proof}
\end{exercise}


\begin{exercise}{2.6.7}
\emph{Solved in Class}
\end{exercise}

\begin{exercise}{2.6.8}
If $\{a_n\}$ and $\{b_n\}$ are non-negative sequences and $\{b_n\}$ converges, prove that $\lim \sup a_n b_n= (\lim \sup a_n) (\lim b_n)$.

	\begin{proof}
	We need to consider two cases. First, assume $\{a_n\}$ is \emph{not} bounded above. Then $\lim \sup a_n  = \infty$. It then doesn't matter what we multply $a_n$ with, we will always get infinity provided that $b_n \neq 0$.Then $\lim \sup (a_n b_n) = \lim \sup a_n \lim b_n = \infty$. \\
	We now consider case 2, where $a_n$ is bounded above. By Bolzano-Weierstrass we know that $a_n$ then has at least one convergent subsequence. Let $a$ be the subsequential limit of $a_n$, and let $M$ be the upper bound of $a_n$. We know then that $\lim \sup a_n$ exists and $\lim \sup a_n \leq M$. \emph{MORE WORK NEEDED ON THIS}. We note that according to the main limit theorem, if $a_n \to a$ and $b_n \to b$, $a_n b_n \to a b$. Thus $\lim \sup (a_n b_n) = \lim \sup a_n \lim b_n$.
	\end{proof}
\end{exercise}


\begin{exercise}{2.6.9}
\emph{Solved in class}
\end{exercise}


\begin{exercise}{2.6.12}
Which numbers do you think are subsequential limits of $\{ \sin n\}_{n=1}^\infty$? Can you prove that your guess is correct? \\

All $x \in R$ with $|x| \leq 1$ are limits for $\sin$.
\end{exercise}



\chapter{Continuous Functions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Continuity}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{exercise}{3.1.1}
If $f$ is a function with domain $[0,1]$, what is the domain of $f(x^2-1)$?\\
=======
    If $f$ is a function with domain $[0,1]$, what is the domain of $f(x^2 -1)$? \\

    $g$ is defined at point $x$ iff $x^2-1 \in [0,1]$, $0 \leq x^2-1 \leq 1$.
    \begin{align*}
        \begin{cases}
            x^2-1 & x\in (-\infty,-1] \cup [1,\infty] \\
            x^2 \leq 2 & x \in [-\sqrt{2},\sqrt{2}]
         \end{cases}
    \end{align*}
    Thus $x \in [-\sqrt{2},-1] \cup [1,\sqrt{2}]$.
\end{exercise}

\begin{exercise}{3.1.2}
What is the natural domain of the function $\frac{x^2 +1}{x^2-1}$? With this as its domain, is this function continuous? Why? \\

The domain is $\R \backslash \{-1,1\}$. The function is continuous everywhere except for the points not part of the domain.
\end{exercise}

\begin{exercise}{3.1.3}
For any real $x$, ${x^2+1}$ is defined and nonzero. And, $1+x^2$ is continuous on $\mathbb{R}$, so by theorem 3.1.9(d), it is continuous.
\end{exercise}

\begin{exercise}{3.1.4}
Show that the function $f(x) = |x|$ is continuous on all of $\R$.

	\begin{proof}
	We need to find a $\delta$ such that for any $\epsilon > 0$, we have $||x| - |a|| < \epsilon$ whenever $|x-a|< \delta$.
	\end{proof}
\end{exercise}

\begin{exercise}{3.1.5}
Assuming $\sin$ is continous, prove that $\sin(x^3-4x)$ is continuous.

	\begin{proof}
	We know that $|sin(x)| < 1$ for all $x$.
	\end{proof}
\end{exercise}

\begin{exercise}{3.1.6}
Since $f(x), g(x)$ are continuous, and $\{f(x_n)\}$ converges to $f(a)$ and $\{g(x_n)\}$ converges to $g(a)$, when $\{x_n\}$ converges to $a$. By theorem 2.3.6(d), $\{f(x_n)/g(x_n)\}$ converges to $f(a)/g(a)$ since $g(a) \neq 0$ by assumption and $g(x_n) \neq 0$ in the domain of $f(x)/g(x)$ by definition.
\end{exercise}

\begin{exercise}{3.1.7} Let $\{x_n\}$ by a sequence in $D_{f \circ g}$. Then $x_n \in D_g$ so if $x_n \rightarrow a,$ $g(x_n) \rightarrow g(a)$. But all the $x_n, a \in D_{f \circ g}$ implies $x_n, a \in D_g$ and $g(x_n), g(a) \in D_f.$ Hence $\{g(x_n)\}$ is a sequence in $D_f$ which converges to $g(a)$ when $\{x_n\}$ converges to $a$, hence $\{f(g(x_n))\}$ converges to $\{f(g(a))\},$ so by theorem 3.1.6 it is continuous.
\end{exercise}

\begin{exercise}{3.1.8}
    We know $\sqrt{x}$ is continuous at all $a \geq 0$ by theorem 3.1.7. Give another proof of this fact by using only the definition of continuity.

    \begin{proof}
    We need to distinguish between two cases: \\
    Case 1 - $a= 0$: $|\sqrt{x} - \sqrt{0}| = \sqrt{x} < \epsilon$ iff $0 \leq x < \epsilon^2$, $\delta=\epsilon^2$. Whenever $x<\epsilon^2$ we find that $\sqrt{x}< \epsilon$ and therefore $\sqrt{x}$ is continuous at $a=0$. \\
    Case 2 - $a>0$: $|x-a|=|\sqrt{x}-\sqrt{a}| |\sqrt{x}+\sqrt{a}|$. This implies $|\sqrt{x}-\sqrt{a}| = \frac{x-a}{\sqrt{x}-\sqrt{a}} \leq \frac{|x-a|}{\sqrt{a}} < \epsilon$ if we have $|x-a| < \epsilon \sqrt{a}$, $\delta=\epsilon \sqrt{a}$.
    \end{proof}
\end{exercise}

\begin{exercise}{3.1.9}
Consider the function:
	\begin{align*}
	f(x) = \left\{
     \begin{array}{lr}
       1 & : x \geq 0,\\
       -1 & : x < 0
     \end{array}
   \right.
	\end{align*}
Is this function continuous if its domain is $\R$? Is it continous if its domain is cut down to $\{ x\in \R: x \geq 0 \}$? How about if its domain is $\{x \in \R : x \leq 0 \}$?
\end{exercise}

\begin{exercise}{3.1.10}
Let $f$ be a function with domain $D$ and suppose $f$ is continuous at some point $a \in D$. Prove that, for each $\epsilon >0$, there is a $\delta >0$ such that
	\begin{align*}
	|f(x) - f(y) | < \epsilon \text{ whenever } x,y \in D \cap (a-\delta,a+ \delta)
	\end{align*}
\end{exercise}

\begin{exercise}{3.1.11}
    Prove that the function
        \[ f(x)= \begin{cases} \sin (1/x) & x\neq 0 \\ 0 & x =0 \end{cases} \]
    is not continuous at 0.

    \begin{proof}
    Whenever $x_n \to 0$ we have $f(x_n) \to f(0) = 0$. We are looking for a sequence $x_n \to 0$ but where $f(x_n) \not\to f(0)=0$. We choose $x_n = \frac{1}{\pi/2 + 2 \pi n}$. This goes to 0 but $\sin(\frac{\pi}{1} +2n)=1$.
    \end{proof}
\end{exercise}



\begin{exercise}{3.1.12}
    Prove that the function
        \[ f(x)= \begin{cases} x \sin(1/x) & x \neq 0 \\ 0 & x=0 \end{cases} \]
    is continuous at 0.

    \begin{proof}
    We need to estimate $|f(x)-f(0)|$.
    \[ |f(x)-f(0)| = |x \sin \left(\frac{1}{x} \right)| = |x| |\sin \left( \frac{1}{x} \right)| \leq |x| < \epsilon \]
    Thus $|x-0|<\epsilon$ for $\delta = \epsilon$.
    \end{proof}
\end{exercise}







\begin{exercise}{3.1.8}
We know $\sqrt{x}$ is continuous at all $a \geq 0$, by Theorem 3.1.7. Give another proof of this fact using only the definition of continuity (Def. 3.1.1). \\

\emph{solved in class}
\end{exercise}


\begin{exercise}{3.1.11}
Prove that the function (piecewise function) is not continuous at 0.\\

\emph{solved in class}
\end{exercise}

\begin{exercise}{3.1.12}
Prove that the function (piecewise function) is continuous at 0.\\

\emph{solved in class}
\end{exercise}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Properties of Continuous Functions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{exercise}{3.2.1}
The minimum of $f$ on $[0, 3)$ is $-1$. The maximum does not exist, since $\sup f_{[0,3)} = 6$ which is achieved at $x=3 \not \in [0,3)$.
\end{exercise}

\begin{exercise}{3.2.2}
    Prove that if $f$ is a continuous function on a closed bounded interval $I$ and if $f(x)$ is never 0 for $x \in I$, then there is a number $m>0$ such that $f(x)\geq m$ for all $x \in I$ or $f(x) \leq -m$ for all $x \in I$.

    \begin{proof}
    Assume $f(a)>0$. We have that $f([a,b])=[m,M]$. We know that $m=\min f$, $M=\max f$. Let's prove that $m>0$. By contradiction: assume $m<0$. Value 0 is taken \emph{(non-legible)} $a_n$ intermediate value $[m, f(a)]$ which contradicts $f(x)\neq 0$. Prove for case 2 is analogous (show that $M <0$).
    \end{proof}
\end{exercise}

\begin{exercise}{3.2.3}
    Prove that if $f$ is a continuous function on a closed bounded interval $[a,b]$ and if $(x_0, y_0)$ is any point in the plane, then there is a closest point to $(x_0,y_0)$ on the graph of $f$.

    \begin{proof}
    Pick any point $x \in [a,b]$. Then the distance to $x_0,y_0$ is $\text{dist}\left( (x_0,y_o), (x,f(x)) \right) = \left( (x-x_0)^2 + (y_0-f(x))^2)\right)^{\frac{1}{2}}$. We must prove that this function attains its minimum value in $[a,b]$ and that if $f$ is continuous, then $\left( (x-x_0)^2 + (y_0-f(x))^2)\right)^{\frac{1}{2}}$ is also continuous on $[a,b]$. Then distance takes its minimum value there.
    \end{proof}
\end{exercise}


\begin{exercise}{3.2.4}
Find an example of a function which is continuous on a bounded (but not closed) interval $I$, but is not bounded. Then find an example of a function which is continuous and bounded on a bounded interval $I$, but does not have a maximum value. \\

The function $f: (0,1) \to \R$ with $f(x)=\frac{1}{x}$ fulfills the first condition. \\
The second condition cannot be fulfilled; according to theorem 3.2.1 (p. 65): ``If $f$ is a continuous function on a closed bounded interval $I$, then $f$ is bounded on $I$ and in fact, it assumes both a minimum and a maximum value on $I$.'' The only way to create a function which would \emph{not} assume a maximum on such an interval would be by violating the continuity. For example, the function
	\begin{align*}
	f(x) =
		\begin{cases}
		2x & x< 1/2 \\
		0  & x \geq 1/2
		\end{cases}
	\end{align*}
fails to achieve its maximum on a bounded interval $[0,1]$. However, it does so by having a discontinuity at $x = 1/2$.
\end{exercise}

\begin{exercise}{3.2.5}
Let $f(x) = e^x$, $I = [1, \infty)$. Then $f$ is continuous on a closed (but not bounded) interval, but is not bounded. If $f(x) = 1/(1+e^{-x})$ and $I = [0, \infty)$, then $f$ is continuous and bounded by $1$ on $I$ which is closed but does not have a maximum.
\end{exercise}


\begin{exercise}{3.2.7}
Give an example of a function defined on the interval $[0,1]$ which does not take on every value between $f(0)$ and $f(1)$. \\

In other words, we are looking for a function with a discontinuity between $[0,1]$. One example would be:
 \begin{displaymath}
   f(x) = \left\{
     \begin{array}{lr}
       x & : 0 \leq x < \frac{1}{2} \\
       2x & : \frac{1}{2} \leq x \leq 1
     \end{array}
   \right.
\end{displaymath}

\end{exercise}


\begin{exercise}{3.2.8}
    Show that if $f$ and $g$ are continuous functions on the interval $[a,b]$ such that $f(a)< g(a)$ and $g(b)<f(b)$, then there is a number $c \in (a,b)$ such that $f(c)=g(c)$.

    \begin{proof}
    We create a function $h(x)=f(x)-g(x)$. This is continuous since it is a linear combination of continuous functions, and it is defined on $[a,b]$. We know that $h(a)=f(a)-g(a)<0$ and $h(b)=f(b)-g(b)>0$.  Bt the intermediate value theorem there exists a $c$ such that $h(c)=f(c)-g(c)=0$, which implies $f(c)=g(c)$.
    \end{proof}
\end{exercise}



\begin{exercise}{3.2.9}
Let $f$ be a continuous function from $[0,1]$ to $[0,1]$.Prove that there is a point $c \in [0,1]$ such that $f(c) = c$ - that is, show that $f$ has a \emph{fixed point}. Hint: Apply the Intermediate Value Theorem to the function $g(x) = f(x)-x$.
	\begin{proof}
	Let $g(x) = f(x) -x$. Since $f(x)$ is continuous, we know that $g(x)$ is also continuous. Then $g(a) \geq 0$ and $g(b) \leq 0$. By the intermediate value theorem we know that there exists some $x \in [0,1]$ such that $g(x) = 0$, which implies that $f(x)=x$.
	\end{proof}
\end{exercise}


\begin{exercise}{3.2.10}
    Use the intermediate value theorem to prove that if $n$ is a natural number, then every positive number $a$ has a positive $n$-th root.

    \begin{proof}
    We write the function $f(x)=x^n$ which is continuous on $[0,\infty)$ since it is a polynomial. We notice $f(0)=0<a$. We know that there is a number $m\in \N$ such that $m>a$ which implies $f(m)=m^n \geq m > a$. Thus we have $f(0) < a$ and $f(m)>a$ and since $f$ is continuous on $[0,m]$, the intermediate value theorem states that there exists a $c$ such that $f(c)=c^n=a$.
    \end{proof}
\end{exercise}


\begin{exercise}{3.2.11}
Prove that a polynomial of odd degree has at least one real root.

	\begin{proof}
	Assume $g(x): \R \to \R$ is an odd degree polynomial. Then $g$ is of the form $\sum_{k=0}^{n} a_k x^k$, where $a_k$ is the $k$-th coefficient of the polynomial an $n \in \N$ such that $n \mod 2 = 1$. We can then factor $g$ to be of the form $g(x)= x^n (a_n + \sum_{k=0}^{n-1} a_k \frac{x^k}{x^n} )$. We note that $\lim_{x \to \pm \infty} \sum_{k=0}^{n-1} a_k \frac{x^k}{x^n}  = 0$. We then consider $\lim_{x \to \pm \infty} x^n a_n$. We note that since $n$ is odd, $x^n \leq 0$ if $x \leq 0$ and $x^n \geq 0$ if $x \geq 0$. Therefore, $\lim_{x \to +\infty} x^n a_n = + \infty$ and $\lim_{x \to -\infty} x^n a_n = -\infty$, provided that $a_n > 0$. Hence, we find that $\lim_{x \to - \infty} g(x) = - \infty$ and $\lim_{x \to \infty} g(x) = \infty$. In the case of $a_n < 0$, we find that $\lim_{x \to -\infty} g(x) = \infty$ and $\lim_{x \to \infty} g(x)= -\infty$. \\
	We know that any polynomial is continous, and the above shows that there are some $a,b \in \R$ such that $g(a) < 0$ and $g(b) > 0$. We now consider the interval $[a,b]$. By the Intermediate value theorem, we find that for every $c \in [g(a),g(b)]$ there exists some $x \in [a,b]$ such that $g(x)=c$, implying that there exists at least one $x$ such that $g(x)=0$.
	\end{proof}
\end{exercise}



\begin{exercise}{3.2.12}
Use the Intermediate Value Theorem to prove that $f$ is a continuous function on an interval $[a,b]$ and if $f(x) \leq m$ for every $x \in [a,b)$, then $f(b) \leq m$.
	\begin{proof}
	Assume that $m < f(b)$, such that $f(x) \leq m < f(b)$ for all $x \in [a,b)$. We then know that $m = f(b)- \delta$ for some $\delta > 0 \in \R$. But, by properties of the real numbers, we would also have $m = f(b) - \delta < f(b) - \epsilon < f(b)$ some some $\epsilon$, such as $\epsilon = \frac{\delta}{2}$. But $f(b)- \epsilon \in [a,b)$ - contradiction: by the intermediate value theorem, since $f$ is continuous, we know that there exists some $x$ such that $f(x)= f(b) - \epsilon$, and thus we require $m \geq f(b) - \epsilon$. Hence, $f(b) \leq m$.
	\end{proof}
\end{exercise}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Uniform Continuity}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{exercise}{3.3.1}
Is the function $f(x)=x^2$ uniformly continuous on $(0,1)$? Justify your answer. \\

Yes, it is. According to Theorem 3.3.4, if a function is continuous on a closed bounded interval $I$, it is uniformly continuous there. Assume $I=[0,1]$. Then by theorem 3.3.4, $f$ is uniformly continuous on $I$. By theorem 3.3.6, $f$ is then also uniformly continuous on $(0,1)$.
\end{exercise}


\begin{exercise}{3.3.2}
    Is the function $f(x)=1/x^2$ uniformly continuous on $(0,+\infty)$ (\emph{actual text printed asks only about interval up to 1})? Justify your answer. \\

    Assume $f$ were uniformly continuous. Then it it is uniformly continuous on a subinterval, such as $(0,1)$. But $f(x)$ is \emph{not} bounded on the interval $(0,1)$. Therefore, it is not uniformly continuous.
\end{exercise}


\begin{exercise}{3.3.3}
Is the function $f(x) = x^2$ uniformly continuous on $(0, + \infty)$? Justify your answer.  \\

No, it its not. As $x \to \infty$ we find that the distance between $y,y'$ gets bigger and bigger, such that $x,x'$ need to be closer and closer for $y,y'$ to still be within $\epsilon$ of each other. This means that $\delta$ does depend on $a$, so it is not uniformly continuous.
\end{exercise}


\begin{exercise}{3.3.4}
    Using only the $\epsilon-\delta$ definition of uniform continuity, prove that the function $f(x)=\frac{x}{x+1}$ is uniformly continuous on $[0,\infty)$.

    \begin{proof}
        \begin{align*}
        |f(x)-f(y)| &= | \frac{x}{x+1} - \frac{y}{y+1} | = |\frac{x(y+1) + y(x+1)}{(x+1)(y+1)}| \\
         &= |\frac{xy+x-xy-y}{(x+1)(y+1)}| = \frac{|x-y|}{(x+1)(y+1)} \leq |x-y|
         \end{align*}
    Estimate $|f(x)-f(y)|\leq |x-y|$. Then for all $\epsilon> 0$, $\delta = \epsilon$ implies $|x-y|<\delta = \epsilon$ will result in $|f(x)-f(y)|<\epsilon$.
    \end{proof}
\end{exercise}



\begin{exercise}{3.3.5}
    In example 3.3.8 we showed that $\sqrt{x}$ is uniformly continuous on $[1,\infty)$. Show that it is also uniformly continuous on $[0,1]$. \\

    By theorem 3.3.4: if $\sqrt{x}$ is continuous on $[0,1]$, it is uniformly continuous there.
\end{exercise}


\begin{exercise}{3.3.6}
Prove that if $I$ and $J$ are overlapping intervals in $\R (I \cap J \neq \emptyset$ and $f$ is a function, defined on $I \cup J$, which is uniformly continuous on $I$ and uniformly continuous on $J$, then it is also uniformly continuous on $I \cup J$. Use this and the previous exercise to prove that $\sqrt{x}$ is uniformly continuous on $[0, + \infty)$.

	\begin{proof}
	By assumption $I \cap J \neq \emptyset$. We shall assume that the interval $I$ is the ``lower'' one of the two. Then there exists an $x$ such that $x \in I \cap J$. Since $I$ is uniformly continuous by assumption, we know that $[x-a,x]$ is uniformly continous for all $(x-a)\in I$. Likewise we know that $[x,x+b]$ is uniformly continous for all $(x+b)\in J$ since $J$ is uniformly continous by assumption. This implies that the whole interval $[x-a, x+b]$ is uniformly continous. \\
	To prove that $\sqrt{x}$ is uniformly continuous, we assume we are given some $\epsilon >0$. For this, we pick $\delta=\epsilon^2$. We note that $|\sqrt{x} - \sqrt{y}| \leq |\sqrt{x} + \sqrt{y}|$. If $|x-y|< \delta = \epsilon^2$, we find:
		\begin{align*}
		|\sqrt{x} - \sqrt{y}|^2 \leq |\sqrt{x} - \sqrt{y}| |\sqrt{x}+\sqrt{y}| = |x-y| < \epsilon^2
		\end{align*}
	This guarantees that $|\sqrt{x} - \sqrt{y}|<\epsilon$, proving that $\sqrt{x}$ is uniformly continous on $(0,\infty)$.
	\end{proof}
\end{exercise}

\begin{exercise}{3.3.7}
Suppose $f$ is not continuous on $I$, then since uniform continuity implies continuity, $f$ is not uniform continuous on $I$. Suppose $f$ is continuous on $I$. Then $f$ is uniform continuous on $I$ if $f$ is continuouss on $\bar{I}$. But a continuous function on a closed, bounded interval is bounded, but $f$ is unbounded. This is a contradiction so $f$ cannot be uniformly continuous.
\end{exercise}



\begin{exercise}{3.3.8}
Let $f$ be a function defined on an interval $I$ and suppose that there are positive constants $K$ and $r$ such that
	\[ |f(x)-f(y)| \leq K |x-y|^r \text{ for all } x,y \in I. \]
Prove that $f$ is uniformly continuous.

	\begin{proof}
	According to assumption, we find that $|f(x)-f(y)| \leq K |x-y|^r$ for all $x,y \in I$. This implies that if $K | x-y|^r < \epsilon$, $|f(x)-f(y)|<\epsilon$. Thus we find that we need to solve $K |x-y|^r < \delta \leq \epsilon$, and find that for any given $\epsilon$, we pick a $\delta$ such that $\delta = \sqrt[r]{\frac{\epsilon}{K}}$. Since $\delta$ does not depend on where $x,y$ are in the interval, $f$ is uniformly continuous.
	\end{proof}
\end{exercise}



\begin{exercise}{3.3.9}
Is the function $f(x)=\sin(\frac{1}{x})$ continuous on $(0,1)$? Is it uniformly continuous on $(0,1)$? Justify your answers.

	\begin{proof}
	Since $\sin$ is a trigonometric function, it is continuous on its whole domain. Likewise, $\sin(1/x)$ is continous since it is merely a composition of two elementary functions. \\
	However, $\sin(1/x)$ is \emph{not} uniformly continous. The reason for this is that the as $x \to 0$ the function oscillates between $-1$ and $1$. Thus, a $\delta$ that would work at one point in the function will can produce potentially a difference $|f(x)-f(y)|=2$ for $x,y$ sufficiently close to 0. Hence, the functions is not uniformly continous.
	\end{proof}
\end{exercise}


\begin{exercise}{3.3.10}
    Is the function $f(x)=x \sin(1/x)$ uniformly continuous on $(0,1)$? Justify your answer. \\

    \begin{proof}
    Method 1: $f(1)=\sin(1)$. It is still uniformly continuous. $\lim_{x \to 0} f(x)= \lim_{x \to 0} x \sin(1/x) = 0$. By squeeze theorem:
		\begin{align*}
		\underbrace{0}_\text{$\to$ 0} \leq \overbrace{|x \sin \left( \frac{1}{x} \right) |}^\text{$\to$ 0 by squeeze thrm.} \leq \underbrace{|x|}_\text{$\to$ 0}
		\end{align*}

    If we define $f(0)=0$, $f(1)=\sin(1)$, then $f(x)$ becomes continuous on $[0,1]$. then by theorem 3.3.4, $f$ is uniformly continuous on $[0,1] \implies f$ is uniformly continuous on $(0,1)$.\\
    Method 2:
        \[ |f(x)-f(y)| = |x \sin(1/x) - y \sin(1/y)| \leq |x \sin(1/x)| + |y \sin(1/y)| \leq |x|+|y| \]
    Then for all $\epsilon>0$ we have $|f(x)-f(y)|<\epsilon$ if $x,y \in (0,\frac{\epsilon}{2}]$. If now $x,y > \epsilon/3$, then there exists a $\delta>0$ such that $|f(x)-f(y)| <\epsilon$ whenever $|x-y| < \delta$:
        \[ |f(x)-f(y)| \leq |x|+|y| < \frac{\epsilon}{3} + |x| + |y-x| < \frac{\epsilon}{3} + \frac{\epsilon}{3}+ \delta < \epsilon \]
    if $\delta< \frac{\epsilon}{3}$. Then we choose $\delta= \min(\frac{\epsilon}{3}, \delta_0)$.
    \end{proof}
\end{exercise}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Uniform Convergence}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\chapter{The Derivative}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Limits of Functions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Derivative}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Mean Value Theorem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{exercise}{4.3.1}
\end{exercise}

\prb{4.3.1} If we apply the MVT to the intervals $[-1, 1]$, $[0, 1]$ and $[-1, 0]$ we find that there exist points where the derivative equals $1/2, 1,$ and $0$.

\prb{4.3.2} Observe that $f(x) = \sin x$ satisfies the hypotheses of theorem 4.3.9 where we let $(a,b) = (-\infty, \infty)$ and so it must satisfy $|f(x) - f(y)| \leq M |x-y|$ where $M$ is a bound for the derivative. In this case $f'(x) = \cos x \leq 1$ for all $x$, hence we may set $M = 1$. Then $|\sin x - \sin y| = |f(x) - f(y)| \leq M |x-y| = |x-y|$ as desired.

\begin{exercise}{4.3.3}
\end{exercise}

\prb{4.3.4} We know that there is some $c \in (0, \infty)$ with $f(x) - f(y) = f'(c) (x-y)$ for $x, y \in (0, \infty)$. Then $|f(x) - f(y)| \leq f'(c) |x-y| \leq M|x-y|$. If we take the limit of both sides as $y \rightarrow 0$, then since $|f(x) - f(y)|$ is continuous, $\displaystyle \lim_{y \rightarrow 0} |f(x) - f(y)| = |f(x) - f(0)| = |f(x)|$. Hence $|f(x)| \leq Mx$.

\begin{exercise}{4.3.5}
\end{exercise}

\prb{4.3.6} We know that $f'(x) = 6x^2 + 6x - 12 = 6(x+2)(x-1)$. Hence $f'(x) \leq 0$ on $[-2, 1]$ and is positive elsewhere. Hence $f(x)$ is decreasing on $[-2, 1]$ and increasing elsewhere.

\begin{exercise}{4.3.7}
\end{exercise}


\begin{exercise}{4.3.8}
\end{exercise}


\begin{exercise}{4.3.9}
\end{exercise}


\begin{exercise}{4.3.10}
\end{exercise}


\begin{exercise}{4.3.11}
\end{exercise}

\begin{exercise}{4.3.12}
\end{exercise}


\begin{exercise}{4.3.13}
\end{exercise}


\begin{exercise}{4.3.14}
\end{exercise}

\begin{exercise}{4.3.15}
\end{exercise}

\begin{exercise}{4.3.16}
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{L'Hopital's Rule}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\prb{4.4.1} By Cauchy's form of the MVT for the interval $[1, x]$ and $f(x) = \ln x$ and $g(x) = x^r$, we have $$\dfrac{\ln x}{x^r - 1} = \dfrac{1/c}{rc^{r-1}} = \dfrac{1}{rc^r},$$ hence $$\ln x = \dfrac{x^r - 1}{rc^r}.$$ We know moreover that $c > 1$ and $r > 0$, hence $r \geq \log_c 1,$ or $c^r \geq 1$ hence $1/c^r \leq 1.$ Then $$\ln x = \dfrac{x^r - 1}{rc^r} \leq \dfrac{x^r-1}{r},$$ as desired.


\begin{exercise}{4.4.2}
\end{exercise}



\begin{exercise}{4.4.3}
\end{exercise}



\begin{exercise}{4.4.4}
\end{exercise}




\begin{exercise}{4.4.5}
\end{exercise}



\prb{4.4.6} The limit is of the form $\infty/\infty$, hence \begin{align*}
\lim_{x \rightarrow \infty} \dfrac{\ln x}{x^r} & = \lim_{x \rightarrow \infty} \dfrac{1/x}{rx^{r-1}} \\
& = \lim_{x \rightarrow \infty} \dfrac{1}{rx^{r}} = 0. \end{align*}

\prb{4.4.7} If we write $x \ln x = \dfrac{\ln x}{1/x}$ then the limit is of the form $-\infty/\infty$. By L'Hopital's, the limit is equal to $$\lim_{x \rightarrow 0} \dfrac{1/x}{-1/x^2} = \lim_{x \rightarrow 0} -x = 0.$$

\prb{4.4.8} The limit is of the form $0/0$ hence it is equal to $$\lim_{x \rightarrow 0} \dfrac{\cos x - 1}{3x^2}.$$ It is still of the form $0/0$ hence equal to $$\lim_{x \rightarrow 0} \dfrac{-\sin x }{6x}.$$ Applying L'Hopital's again, the limit equals $$\lim_{x \rightarrow 0} \dfrac{-\cos x }{6} = 0.$$



\begin{exercise}{4.4.9}
\end{exercise}


\prb{4.4.10} Let $g(x) = \log f(x) = x \log x.$ Then $$\lim_{x \rightarrow 0} x \log x = \lim_{x \rightarrow 0} \log x /(1/x).$$ This has numerator and denominator $\pm \infty$, hence we may apply L'Hopital's to get $$\lim_{x \rightarrow 0} x \log x = \lim_{x \rightarrow 0} (1/x)/(-1/x^2) = \lim_{x \rightarrow 0} - x = 0.$$


\begin{exercise}{4.4.11}
\end{exercise}

\begin{exercise}{4.4.12}
\end{exercise}


\begin{exercise}{4.4.13}
\end{exercise}


\begin{exercise}{4.4.14}
\end{exercise}

\begin{exercise}{4.4.15}
\end{exercise}

\begin{exercise}{4.4.16}
\end{exercise}



\chapter{The Integral}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Definition of the Integral}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\prb{5.1.1} We have $$U(f, P) = 1 \cdot \dfrac{1}{4} + \dfrac{4}{5} \cdot \dfrac{1}{4} + \dfrac{2}{3} \cdot \dfrac{1}{4} + \dfrac{4}{7} \cdot \dfrac{1}{4} = 319/40.$$ Moreover, $$L(f,P) = \dfrac{4}{5} \cdot \dfrac{1}{4} + \dfrac{2}{3} \cdot \dfrac{1}{4} + \dfrac{4}{7} \cdot \dfrac{1}{4} + \dfrac{1}{2} \cdot \dfrac{1}{4} = 533/840.$$

\prb{5.1.2} Since $x$ is an increasing function, we know that $M_k = k/n$ and $m_k = (k-1)/n$ hence $$U(f, P_n) = \displaystyle \sum_{k=1}^n \dfrac{k}{n} \dfrac{1}{n} = \dfrac{1}{n^2} \dfrac{n(n+1)}{2} = \dfrac{n+1}{2n}$$ and $$L(f, P_n) = \displaystyle \sum_{k=1}^n \dfrac{k-1}{n} \dfrac{1}{n} = \dfrac{1}{n^2} \dfrac{n(n-1)}{2} = \dfrac{n-1}{2n}.$$ Since these have the same limit as $n \rightarrow \infty$, we know that $\lim_{n \rightarrow \infty} U(f, P_n) - L(f, P_n) = 0,$ as desired.

\prb{5.1.3} The base case is clearly true, so suppose that $$\displaystyle \sum_{j=1}^k j^2 = \dfrac{k(k+1)(2k+1)}{6}.$$ Then adding $(k+1)^2$ to both sides yields $$\displaystyle \sum_{j=1}^{k+1} j^2 = \dfrac{k(k+1)(2k+1)}{6} + (k+1)^2 = \dfrac{(k+1)(k+2)(2(k+1)+1)}{6},$$ as desired.

\prb{5.1.4} By using exercise 3 above, one finds that $$U(f, P_n) = \sum_{k=1}^n \left(\dfrac{ka}{n}\right)^2 \dfrac{a}{n} = \dfrac{a^3}{n^2} \dfrac{n(n+1)(2n+1)}{6},$$ where $P_n = \{ 0 < \dfrac{a}{n} < \cdots < \dfrac{(n-1)a}{n} < a\}$ and $M_k = (ka/n)^2$ since $f(x) = x^2$ is increasing. Moreover, notice that the limit is $a^3/3$. We now find that $$L(f, P_n) = \sum_{k=1}^n \left(\dfrac{(k-1)a}{n}\right)^2 \dfrac{a}{n}$$ which we may find evaluates to $$\dfrac{a^3}{n^2} \dfrac{n(n-1)(2n-1)}{6}.$$ Notice that this also has limit $a^3/3$, hence the limit of the difference of the two is zero, and the integral exists. Moreover, it equals $\lim U(f, P_n) = a^3/3.$

\prb{5.1.5} For any $P$, we know that $U(f,P) = 1$ since $U(f, P) = \displaystyle \sum_k M_k (x_k - x_{k-1}) = \sum_k x_k - x_{k-1} = 1-0 = 1$ as the supremum of $f$ on any interval is $1$. On the other hand we know $L(f,P) = 0$ since the infimum of $f$ on any interval is $0$. Hence for all partitions $P$, $U(f,P) - L(f,P) = 1$, hence by theorem 5.1.7 the function is not Riemann integrable.

\prb{5.1.8} We know that $$\underline{\int_a^b} f(x) \ dx = \sup \{L(f, Q): Q \in \mathcal{P}\},$$ where $\mathcal{P}$ is the set of partitions of $[a,b]$. This is equal to $\sum_{k=1}^n m_k (x_k - x_{k-1})$ for some partition $\{ a = x_0 < x_1 < \cdots < x_n = b\}$ of $[a,b]$. But we know $$\sum_{k=1}^n m_k (x_k - x_{k-1}) \geq \sum_{k=1}^n m (x_k - x_{k-1}) = m(b-a).$$ This gives the first inequality in the chain of inequalities. The second inequality is obtained by the definition of supremum and infimum. The third inequality is found by noting that $M_k \leq M$.

\prb{5.1.9} For any partition, $$U(f, P) = \sum_{k=1}^n M_j (x_j - x_{j-1}) = \sum_{j=1}^n k (b-a) = \sum_{j=1}^n m_k (b-a) = L(f, P).$$ Hence $U(f, P) - L(f, P) = 0 < \epsilon$ for any partition $P$, so by theorem 5.1.7 the integral exists. Applying definition 5.1.6 and the definition of upper and lower integral, we find that the integral equals $\sum_{j=1}^n k (b-a)$.

\prb{5.1.10} See definition 5.1.1. If $P$ partitions $[a,b]$ into $n$ equal subintervals, then $x_k - x_{k-1} = (a-b)/n$ hence the difference is $$\dfrac{a-b}{n} \sum_{k=1}^n (M_k - m_k).$$



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Existence and Properties of the Integral}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\prb{5.2.1} If $f = g-h$ and $g, h$ are non-decreasing, then $g', h' \geq 0$. We will then have $g' \geq h'$, $g' = h'$ or $g' \leq h'.$ In the first and third cases, we know that either $f' \geq 0$ or $f' \leq 0$. In either case, $f$ is monotone on a closed bounded interval $[a,b]$, hence by theorem 5.2.1 it is integrable. On the other hand if $g' = h'$ then $f' = 0$, hence $f$ is constant and it was proved that a constant function is integrable in the previous homework.

\prb{5.2.6} We know $1+x^{2n}$ is continuous and never zero, hence $1/(1+x^{2n})$ is also continuous on $[-1, 1]$, hence by theorem 5.2.2 it is integrable. Moreover, $M = \sup_{[-1, 1]} f = 1$ and $m = \inf_{[-1, 1]} f = 1/2$. By corollary 5.2.5, we know that the integral is therefore trapped between $m(b-a) = \frac{1}{2} \cdot 2 = 1$ and $M(b-a) = 1 \cdot 2 = 2.$ Hence the desired inequality.

\prb{5.2.11} Let $f(x) = 1$ if $x \in [0,1]$ is rational, and $-1$ if $x$ is irrational. Then for reasons similar to problem 5.1.5, this function is not integrable. However, $|f(x)| = 1$ for all $x \in [0,1]$, and hence it is integrable.

\prb{5.2.12} We know that $f(x)$ is continuous, hence integrable on a closed bounded interval $[a,b]$. It is also therefore bounded, and so $M = \sup_{[a,b]} f$ and $m = \inf_{[a,b]} f$ are finite so $f$ actually attains these values. Moreover, we know by the IVT that $f$ attains every value between $m$ and $M$ (possibly these values). In particular, $f$ attains the value $$\dfrac{1}{b-a} \int_a^b f(x) \ dx,$$ by corollary 5.2.5.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Fundamental Theorems of Calculus}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\prb{5.3.1} We know that the antiderivative is $f(x) = x^2 \sin(1/x)$ and that it satisfies all the hypotheses of theorem 5.3.1 from example 5.3.2, and so the integral equals $f(2/\pi) - f(4/\pi) = 4/\pi^2 - 8\sqrt{2}/\pi^2.$

\prb{5.3.2} It is clear that the derivative is $\cos (1/x).$

\prb{5.3.4} The integral may be split up as $$\int_0^x e^{-t^2} \ dt - \int_0^{1/x} e^{-t^2} \ dt,$$ and hence the derivative of the first integral is $e^{-x^2}$ and of the second it is $-e^{-1/x^2}/x^2$ by the chain rule and second fundamental theorem of calculus.

\prb{5.3.5} $f(x) = -1/x$ is not continuous at $x = 0 \in [-1, 1]$, violating the hypotheses of theorem 5.3.1.

\prb{5.3.6} Letting $g(x) = f(x),$ applying (5.3.5) gives that the integral equals $f(b)^2 - f(a)^2 - \displaystyle \int_a^b (f'(x))^2 \ dx.$

\prb{5.3.8} Let $x \rightarrow t$ and $g'(t) = t^n$ and $f(t) = \ln t$. Then $g(t) = \dfrac{1}{n+1} t^{n+1}$ and $f'(t) = 1/t$. Both $f$ and $g$ are continuous on $[0, \infty)$ and differentiable on $(0, \infty)$. We know $fg'$ and $gf'$ are continuous, hence by (5.3.5) the integral equals $$\dfrac{1}{n+1} t^{n+1} \ln t \bigg|_0^x - \int_0^x \dfrac{1}{n+1} t^{n+1} \dfrac{1}{t} \ dt.$$ Simplifying this and making the necessary evaluations, the integral comes out as $$\dfrac{1}{n+1} x^{n+1} \ln x - \dfrac{1}{(n+1)^2} x^{n+1}.$$

\prb{5.3.10} Theorem 5.3.1 requires $f(x) = x/|x|$ be continuous at every point in $[-1, 1]$, but $f(x)$ is not continuous at $x = 0.$


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Logs, Exponentials, Improper Integrals}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Infinite Series}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Convergence of Infinite Series}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Tests for Convergence}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Absolute and Conditional Convergence}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Power Series}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{exercise}{6.4.2}
Prove that $f(x) = \sum_{k=1}^\infty \frac{sin (kx)}{2^k} $ is continuous on the entire real line.

\begin{proof}
    We use the Ratio test to show that the sum converges on the entire real line:
        \begin{align*}
        \frac{\sin\left( (k+1)x \right)}{2^k 2} \frac{2^k}{\sin(kx)} = \frac{\sin \left(kx+x\right)}{2 \sin(kx)} \leq \frac{1}{2}.
        \end{align*}
    This proves convergence on all of $\R$. We notice that $\sin(kx)2^{-k} \leq 2^{-k}$. Since $\sum_{k=0}^\infty 2^{-k} = 2$, we know by the Weierstrass M-test that $\sum_{k=1}^\infty \frac{sin (kx)}{2^k}$ converges uniformly on all $\R$. This proves that this sum is continuous on the entire real line.
\end{proof}
\end{exercise}



\begin{exercise}{6.4.4}
Radius of convergence of $\sum_{k=1}^\infty \frac{1}{k 3^k}x^k$. \\

We have $c_k = (k 3^k)^{-1}$. We proceed to take the $k$-th root:
    \[ (k 3^k)^{-1/k} = \frac{1}{3} \frac{1}{\sqrt[k]{k}} = 3^{-1}. \]
We then solve using the formula for the radius of convergence:
    \begin{align*}
    R = \frac{1}{\lim \sup \sqrt[k]{c_k}} = \frac{1}{\lim \sup 3^{-1}} = 3.
    \end{align*}
 Therefore, the radius of convergence is 3.
\end{exercise}



\begin{exercise}{6.4.5}
Radius of convergence of $\sum_{k=0}^\infty \frac{(-1)^{k-1}}{k+1} (x+2)^k$.  \\

We have $c_k= \frac{(-1)^{k-1}}{k+1}$. We take the $k$-th root:
    \begin{align*}
    (c_k)^{1/k} &=  \left( \frac{(-1)^{k-1}}{k+1} \right)^{\frac{1}{k}} = \left(- \frac{(-1)^k}{k+1} \right)^{\frac{1}{k}} \\
                &= - \frac{(-1)^{k/k}}{\left(k+1\right)^{1/k}} = - \frac{-1}{\left(k+1\right)^{1/k}} = \frac{1}{\left(k+1\right)^{1/k}}.
    \end{align*}

    We then use the formula for the radius of convergence:
    \begin{align*}
    R = \frac{1}{\lim \sup \sqrt[k]{c_k}} = \frac{1}{\lim \sup \left(k+1\right)^{-1/k}} = \frac{1}{1} = 1.
    \end{align*}
    And hence, the radius of convergence is 1.
\end{exercise}



\begin{exercise}{6.4.8}
Radius of convergence of $\sum_{k=0}^\infty 2^k x^{2k}$.  \\

We have $c_k= 2^k$. We then take the $k$-th root: $\left( 2^k \right)^{\frac{1}{k}} =2^{k/k} = 2$. We use the formula for the the radius of convergence:
    \begin{align*}
      R = \frac{1}{\lim \sup \sqrt[k]{c_k}} = \frac{1}{\lim \sup 2} = \frac{1}{2}.
    \end{align*}
Thus we can see that the radius of convergence is $1/2$.
\end{exercise}



\begin{exercise}{6.4.10}
Let $\{a_k\}$ be a non-increasing sequence of non-negative numbers which converges to 0. Use theorem 6.3.2 to show that the power series $\sum_{k=0}^\infty (-1)^{k+1} a_k x^k$ converges uniformly on $[0,1]$, and hence converges to a continuous function on this interval.

    \begin{proof}
    According to theorem 6.3.2, the series $\sum_{k=0}^\infty \left(-1\right)^{k+1} a_k$ converges provided that $\{a_k\}$ is a non-increasing sequence of non-negative numbers. We notice that this power series is uniformly Cauchy, since the sequence of partial sums is uniformly Cauchy according to the theorem. Therefore, this series converges uniformly. \\
    Notice that for all $x \in [0,1]$, $\left(-1\right)^{k+1} a_k x^k \leq \left(-1\right)^{k+1} a_k$. Therefore, we see that the series $\sum_{k=0}^\infty (-1)^{k+1} a_k x^k$  is bounded by $\sum_{k=0}^\infty (-1)^{k+1} a_k$, which by the Weierstrass M-test implies that $\sum_{k=0}^\infty (-1)^{k+1} a_k x^k$  is uniformly convergent on $[0,1]$.
    \end{proof}
\end{exercise}



\begin{exercise}{6.4.12}
Prove that if $f(x)$ is the sum of a power series centered at $a$ and with radius of convergence $R$, then $f$ is infinitely differentiable on $(a-R,a+R)$ - that is, its derivative of order $m$ exists on this interval for all $m \in \N$.

   \begin{proof}
   Suppose that $f(x) = \sum_{k=0}^\infty c_k \left( x-a\right)^k$. Then, according to theorem 6.4.12, $f(x)$ is differentiable on $\left(a-R, a+R\right)$ and its derivative is $f'(x) = \sum_{k=1}^\infty k c_k \left( x-a\right)^{k-1} $. This means we are left with a $k-1$ degree differentiable polynomial. By induction we see that we can then integrate this power series again. Since we take $k$ to infinity, this means we can take an infinite number of derivatives. Additionally, any polynomial has an infinite number of derivatives: the $k$th derivative is a constant, and all others are zero.
   \end{proof}
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Taylor's Formula}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{exercise}{6.5.1}
Prove that $\lim \frac{x^n}{n!} = 0$ for all $x$.

\begin{proof}
    Since we take the limit as $n \to \infty$, $n$ will take on every value of $\N$. We assume that $x \in \R$. Therefore, at some point we find have that $x \leq n$; at the first point this occurs, we then consider the rational expression $\frac{x}{n} C$ where $C$ is some constant of the form $\frac{x^{n-1}}{\left(n-1\right)!}$. If we consier the next term in the sequence, $\frac{x\, x}{n\, (n+1)}C$, we find that $n \left(n+1\right)\geq x\,x$, since $x\leq n$. Therefore, it is obvious that $\lim_{n \to \infty} \frac{x^n}{n!}=0$ for all $x$.
\end{proof}
\end{exercise}



\begin{exercise}{6.5.3}
Use Taylor's formula to estimate the error if $\cos(x)$ is approximated by $1-\frac{x^2}{2}$ on the interval $[-0.1,0.1]$. \\

We estimate as follows:
    \begin{align*}
       \frac{x^4}{4!} \leq \frac{0.1^4}{4!} \leq 5\times10^{-6}
    \end{align*}
    Thus we can bound the error by $5\times10^{-6}$.
\end{exercise}



\begin{exercise}{6.5.5}
Taylor's formula for $f(x) = \sqrt{ t+x}$ with $a=0$. \\

We begin by taking the first few derivatives of $f(x) = \sqrt{t+x}$.
    \begin{align*}
    f'(x) &= \frac{1}{2 \sqrt{t + x}} \\
    f''(x) &= - \frac{1}{4 \left(t + x\right)^{\frac{3}{2}}} \\
    f^{(3)} (x) &= \frac{3}{8 \left(t + x\right)^{\frac{5}{2}}} \\
    f^{(4)} (x) &= - \frac{15}{16 \left(t + x\right)^{\frac{7}{2}}}
    \end{align*}

    Then the Taylor formula for the above function for the first 5 terms, with remainder, at $a=0$ will be as follows:
    \begin{align*}
    f(x) &= \sum_{n=0}^\infty \frac{f^{(n)}(a)}{n!} (x-a)^n \\
    &\approx \sqrt{t} + \frac{1}{2 \sqrt{t}} x - \frac{1}{4 t^{\frac{3}{2}}}\frac{1}{2} x^2 + \frac{3}{8 t^{\frac{5}{2}}}  \frac{1}{3!}  x^3 - \frac{15}{16 t^{\frac{7}{2}}} \frac{1}{4!}  x^4 + \frac{105}{32 \left(c + t\right)^{\frac{9}{2}}} \frac{1}{5!} x^5 \\
    &= \sqrt{t} + \frac{x}{2 \sqrt{t}} - \frac{1}{8 t^{\frac{3}{2}}} x^2 + \frac{1}{16 t^\frac{5}{2}} x^3 - \frac{5}{128 t^\frac{7}{2}} x^4 + \frac{7}{256 \left(c+t\right)^\frac{9}{2}} x^5 \,.
    \end{align*}
\end{exercise}




\begin{exercise}{6.5.7}
Taylor's formula for $f(x) =\ln (1+x)$ with $a=0$.  \\

We again begin by taking the first four derivatives of $f$.
    \begin{align*}
    f(x) &= \ln (1+x) \\
    f'(x) &= \frac{1}{x + 1} \\
    f''(x) &= - \frac{1}{\left(x + 1\right)^{2}} \\
    f^{(3)} (x) &= \frac{2}{\left(x + 1\right)^{3}} \\
    f^{(4)} (x) &= - \frac{6}{\left(x + 1\right)^{4}}
    \end{align*}
    Then the Taylor formula for the above function for the first 5 terms, with remainder, at $a=0$ will be as follows:
    \begin{align*}
    f(x) &= \sum_{n=0}^\infty \frac{f^{(n)}(a)}{n!} (x-a)^n \\
    &\approx x - \frac{1}{2} x^2 + \frac{2}{6} x^3 - \frac{6}{24} x^4 + \frac{24}{120\left(c+1\right)^5} x^5 \\
    &= x - \frac{1}{2} x^2 + \frac{1}{3} x^3 - \frac{1}{4} x^4 + \frac{1}{5 \left(c+1\right)^5} x^5 \,.
    \end{align*}
\end{exercise}



\begin{exercise}{6.5.11}
If $f$ is an infinitely differentiable function on $(a-r,a+r)$ and there is some constant $K$ such that $\left| f^{(n)} (x)\right| \leq K \frac{n!}{r^n}$ for all $n \in \N$ and all $x \in (a-r,a+r)$, then the Taylor series for $f$ at $a$ converges to $f$ on $(a-r,a+r)$. \\

This appears to be a backwards reasoning argument. We that if $f$ is a convergent taylor series on $\left(a-R, a+R\right)$, then it is infitinely differentiable on that same interval.

We can see that if there is some constant $K$ such that $\left| f^{(n)} (x)\right| \leq K \frac{n!}{r^n}$ for all $n \in \N$ and all $x \in (a-r,a+r)$, then $f^{(n)}(x)$ is always bounded.
\end{exercise}



\begin{exercise}{6.5.13}
If $g(x)=e^{-1/x^2}$ for $x \neq 0$ and $g(0)=0$, show that $g$ is infinitely differentiable on the entire real line but all of its derivatives at 0 are 0. Argue that this means that $g$ cannot be analytic at 0. \emph{Hint: Use the previous exercise to help compute the derivatives of $g$ at 0.} \\

First note that $g'(x)= 2 \frac{e^{-1/x^2}}{x^3}$. The Taylor expansion of $e^{-1/x^2}$ centered around $a$ is of the form $\sum_{k=0}^\infty \frac{f^{(k)}(a)}{k!} \left(x-a\right)^k$. This is a power series where $c_k=\frac{f^{(k)}(a)}{k!}$ and as such is differentiable according to theorem 6.4.12, and infinitely so (cf. exercise 6.4.12). Likewise, $e^{-1/x^2}$ is infinitely differentiable, and all of its diefferentials are clearly defined everywhere except at $x=0$. This is due to the fact that all $n$-th derivatives of $g$ are sums of the form $\sum_{k=1}^n a_k e^{-1/x^2} x^{-(n+k+1)}$. Therefore, analytically $g(x)$ is not defined at 0.

Remember however that $\lim_{x \to 0} \frac{e^{-1/x^2}}{x^n}=0$. Therefore, if we define $g(x)$ and and all of its derivatives to be 0 at 0, $g(x)$ is infinitely differentiable.
\end{exercise}


%%--------------------------------
%%--------------------------------

\part{Multivariable - 3220}

%%--------------------------------
%%--------------------------------



\chapter{Convergence in Euclidean Space}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Euclidean Space}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{exercise}{7.1.5}
Prove that $\left| |x| - |y| \right| \leq \left| x- y \right|$.

    \begin{proof}
    We note that $x = x-y+y$. This implies that $\left|x\right| = \left| x - y + y\right| $. We use the triangle inequality:\
        \begin{align*}
        \left| x - y + y \right| &\leq \left|x-y\right| + |y| &&\text{which is equivalent to}\\
        \left| x \right| &\leq \left|x-y\right| + |y| &&| - |y| \\
        \left| x \right| - \left| y \right| &\leq \left| x- y \right|
        \end{align*}
    This proves the result.
    \end{proof}
\end{exercise}


\begin{exercise}{7.1.6}
Prove that equality holds in the Cauchy-Schwartz inequality if and only if one of the vectors $\vec{u}, \vec{v}$ is a scalar multiple of the other.

    \begin{proof}
    Let $\vec{v}$ be a scalar multiple of $\vec{u}$, such that $\vec{v} = \alpha \vec{u}$. Then
        \begin{align*}
        \vec{u} \cdot \vec{v} = \vec{u} \cdot \alpha \vec{u} &= \alpha u_1^2 + \alpha u_2^2 + \hdots + \alpha u_n^2 \\
         &= \alpha \left( u_1^2 + \hdots + u_n^2 \right) \\
         &= \alpha \left( \sqrt{ \sum_{i=1}^n u_i^2 } \right)^2 = \alpha \left| \vec{u} \right| ^2.
        \end{align*}
    Therefore, we find that $\left| \vec{u} \right|^2 \left|\vec{v}\right|^2 = \left|\vec{u}\right| \alpha \left|\vec{u}\right|^2= \alpha \left| \vec{u} \right|^2$, and hence, equality holds. The proof for the case where $\vec{u}$ is a scalar multiple of $\vec{v}$ is analogous.
    \end{proof}
\end{exercise}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Convergent Sequences of Vectors}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{exercise}{7.2.7}
Let {Xn} be a bounded sequence in  $R^d $ and let  $An $ be a bounded sequence of scalars. Prove that if either sequence has limit 0, then so does the sequence ${AnXn} $

        If $\{\vec{x}_n: n \in \mathbb{N}\}$ converges to the vector $\vec{x}$ and $\{a_n: n \in \mathbb{N}\}$ to a real number $a$ then the sequences of vectors, $\{a_n\vec{x}_n: n \in \mathbb{N}\}$, converges to $a\vec{x}$.

      \begin{proof}

        Let $\varepsilon > 0$. It remains to prove that there is $N \in \mathbb{N}$ such that, for every natural number $n > N$, we have
        $$
        \|a_n\vec{x}_n-a\vec{x}\|< \varepsilon.
        $$
        By the definition of the limit, there is a integer $N_1>0$ such that, when $n > N_1$, we have
        $$
        \|\vec{x}_n-\vec{x}\|< \sqrt{\varepsilon}.
        $$
        Similarly, by the definition of the limit, there is a integer $N_2>0$ such that, when $n > N_2$, we have
        $$
        |a_n-a|< \sqrt{\varepsilon}.
        $$
        We choose $N = \max(N_1, N_2)$. Let $n \in \mathbb{N}$ such that $n > N$. As the inequalities hold we can multiply them to obtain
        \begin{align*}
        \varepsilon=\sqrt{\varepsilon} \sqrt{\varepsilon}&> |a_n-a|\|\vec{x}_n-\vec{x}\| \\
        &=\||a_n-a|\vec{x}_n-|a_n-a|\vec{x}\|
        \end{align*}

        Then

        \begin{align}
          \| a_n x_n - ax\| &\le \|a_n x_n - a x_n\| + \|a x_n - a x\| \\
          &\le |a_n-a| \|x_n\| + |a| \|x_n-x\|\\
          &\le |a_n-a| (\|x_n-x\|+\|x\|) + |a|\|x_n-x\|
        \end{align}
        and note $\|a_n-a\| \to 0$ and $\|x_n-x\| \to 0$.


      \end{proof}

\end{exercise}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Open and Closed Sets}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{exercise}{7.3.1}
Prove that the set $\{(x, y) \in \R^2 : y > 0\}$ is an open subset of $\R^2$.

    \begin{proof}
    According to theorem 7.3.10, a set $A\in \R^d$ is closed if and only if every convergent sequence in $A$ converges to a point $x \in A$. Consider the sequence $\{\frac{1}{n}, \frac{1}{n} \}$. This sequence is clearly contained in the set $A = \{(x, y) \in \R^2 : y > 0\}$. However, $\{\frac{1}{n}, \frac{1}{n} \} \to (0,0)$, and $(0,0) \notin A$. Therefore, the set $A$ is not closed. Since $A$ is not closed, $A$ is an open set.
    \end{proof}
\end{exercise}


\begin{exercise}{7.3.4}
Find the interior, closure, and boundary for the set
\[ A = \{(x, y) \in \R^2 : |(x, y)| < 1\} \cup \{(x, y) \in \R^2 : y = 0, -2 < x < 2\}. \]
The solution is as follows:
    \begin{align*}
    A^\circ &= \{(x, y) \in \R^2 : |(x, y)| < 1\} \cup \{(x, y) \in \R^2 : y = 0, -2 < x < 2\}\\
    \overline{A} &= \{ (x,y) \in \R^2 : |(x,y)| \leq 1 \lor y=0, |x| \leq 2 \} \\
    \partial A &= \{ (x,y) \in \R^2: |(x,y)| = 1 \lor x=-2, y=0  \lor  x=2, y=0  \}
    \end{align*}
\end{exercise}


\begin{exercise}{7.3.6}
Let $A$ be an open set and $B$ a closed set. If $B \subset A$, prove that $A \backslash B$ is
open. If $A \subset B$, prove that $B \backslash A$ is closed.

\begin{proof}
    \textbf{Case 1:} $B\subset A$. Since $B$ is a closed set, for any sequence $\{x_n\} \in B$, there is some $L_i \in B$ such that $\{x_n\} \to L_i$. Since $B \subset A$, we know that $\{x_n\}, L_i \in A$ for all $n,i \in \N$. If $B$ is a proper subset of $A$, then for every $L_i$ there exists some sequence $\{y_n\}$ such that $\{y_n\} \in A\backslash B$ and $\{y_n\} \to L_i$. Since $L_i \in B$, we find that $L_i \notin A\backslash B$. But then not all limit points of $A \backslash B$ are actually in $A \backslash B$, thus showing that $A \backslash B$ is an open set. \\
    \textbf{Case 2:} $A \subset B$. Since $A$ is an open set, $\exists L_i, \{x_n\}$ such that $L_i \in B$, $\{x_n\} \in A$ and $\{x_n\} \to L_i$. Consider now the set $B \backslash A$. Since $L_i \in B$, but $L_i \notin A$, we find that $L_i \in B \backslash A  \, \, \forall i$. Therefore, $B \backslash A$ is a closed set.
\end{proof}
\end{exercise}


\begin{exercise}{7.3.9}
If $A$ and $B$ are subsets of $\R^d$ show that $\overline{A \cup B} = \overline{A} \cup \overline{B}$. Is the analogous statement true for $A \cap B$? Justify your answer. \\

% \begin{figure}[h!]
%   \centering
%     \includegraphics[width=0.9\textwidth]{Sets}
%   \caption{Sets for exercise 7.3.9 and 7.3.15.}
% \end{figure}

% As is evident from Figure 1, this holds only in the first case. For analytical purposes, assume $x \in \overline{A}$. Then we have that $x \in \overline{A} \cup \overline{B}$. \\
% This is not true for intersections. Consider the case of $A= (-\infty, 0)$, $B= (0, \infty)$ in $\R$. Then $A \cap B = \emptyset$, but  $0 \in \overline{A \cap B}$. Hence $\overline{A \cap B} \neq \overline{A} \cap \overline{B}$.
\emph{Existing solution utilizes a graphic. Try to find analytic solution instead.}
\end{exercise}


\begin{exercise}{7.3.15}
If $E$ is a subset of $\R^d$, show that $(\overline{E})^c = (E^c)^\circ$. \\

As is evident from Figure 1, this identity holds.
\end{exercise}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Compact Sets}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{exercise}{7.4.1}
If $K$ is a compact subset of $\R^d$ and $U_1 \subset U_2 \subset \hdots \subset U_k \subset \hdots$ is a
nested upward sequence of open sets with $K \subset \bigcup_k U_k$ , then prove that $K$ is contained in one of the sets $U_k$.



\begin{proof}
    This seems obvious. Since $U_K$ is a nested upward sequence, we have that $U_i \subset U_{i+1}$ for all $i\in N$. If $K \subset \bigcup_k U_k$, then $\forall x_i \in K$ we find that $x_i \in \bigcup_k U_k$. Since $K$ is a compact set, it has a finite subcoverage. This means, there is a finite $n \in N$ such that $\bigcup_n U_n \subset K$. Therefore, there exists a finite $m \in N$ such that $K \subset \bigcup_{n+m} U_{m+n}$.
\end{proof}
\end{exercise}




\begin{exercise}{7.4.4}
Prove that if $K$ is a compact subset of $\R^d$, then $K$ contains a point of maximal norm. That is, there is a point $x_1 \in K$ such that
    \[ |x| \leq |x_1| \text{ for all } x \in K. \]
\emph{Hint:} set $m =\sup\{ |x| : x \in K \}$  and consider the open balls $B_{m-1/n}(0)$.

    \begin{proof}
    A continuous image of a compact space is compact. Consider the space $K \in \R^d$, and let $f:K \to \R$ such that $f(x)=|x|$. This is a continuous function since the space is compact. As a continuous, real valued function, it attains both a maximum and a minimum per 3.2.1. Therefore, there exists an $x_i \in K$ such that $f(x_i) \geq f(x_k)$ for all $k \in \N$. This proves that $\exists \,x : |x| \geq |x_i|$ for all $i \in \N$.
    \end{proof}
\end{exercise}




\begin{exercise}{7.4.6}
Prove that the conclusion of the previous exercise also holds if we only assume that $K$ is a closed subset of $\R^d$. \emph{Hint:} replace $K$ by its intersection with a suitably large closed ball centered at $y$.

    \begin{proof}
    Let $f(x): K \to \R$ such that $f(x) = |x-y|$ where $y \in \R^d$. Since $K$ is a compact set imaged into a compact set, we know that $f$ is a continuous function. As such, this function attains both a minimum and a maximum per 3.2.1. Therefore, there exists a point $x_i$ such that $f(x_i) \leq f(x_k)$ for all $k \in \N$, which proves that there exists a point $x_i$ such that $\left| x_i - y \right| \leq \left| x- y \right| $ for all $x$.
    \end{proof}
\end{exercise}




\begin{exercise}{7.4.9}
Show that it is true that the union of any finite collection of compact
subsets of $\R^d$ is compact, but it is not true that the union of an infinite
collection of compact subsets is necessarily compact. Show the latter
statement by finding an example of an infinite union of compact sets which
is not compact. \\

Any compact set has a finite subcoverage. Having a finite subcoverage - given an open cover - is sufficient for a set to be considered compact. The union of 2 compact sets then has two sets of compact subcoverages. The union of subcoverages is likewise finite. Assume we have $n$ compact sets with finite subcoverage. The union of these $n$ sets with an additional compact set results in the union of a set of finite subcoverages of the union of the $n$ sets together with a finite subcoverage of the $n+1$-th set. This is then evidently also finite. Hence, any finite union of compact sets is also compact. However, an infinite collection is not necessarily compact as this infinite union may lead to the existence of an infinite number of subcoverages, which would make this union no longer compact.
\end{exercise}



\begin{exercise}{7.4.10}
Prove that if $A$ and $B$ are compact subsets of a $\R^d$, then $A \cup B $ and $A \cap B$ are also compact.
\begin{proof}
    Any compact set has a finite open coverage. Hence, there is a finite open cover $U_A$ of $A$ and an open coverage $U_B$ of $B$. We now consider the union $U_A \cup U_B$. Since both coverages are finite, the union is finite as well. This union now is a finite coverage of $A \cup B$. Since $A \cup B$ has a finite coverage, it is also compact.
\end{proof}
\end{exercise}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Connected Sets}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{exercise}{7.5.1}
$A = \{ (x,y) \in \R^2 : |(x,y)| < 1 \} \cup \{ (x,y) \in \R^2 : 1 \leq x \leq 2, y= 0 \}.$
This set is connected, since it is the union of two connected sets whose intersect is non-empty.
\end{exercise}



\begin{exercise}{7.5.3}
$A= \{ (x,y) \in \R^2 : 1 < |(x,y)| < 2 \}$.
This set is an open set in the form of a disk, and as such is connected.
\end{exercise}



\begin{exercise}{7.5.5}
What are the connected components of the complement of the set of integers in $\R$? \\

The connected components of this set consists of all sets $A = \{ x \in \R : m < x < m+1 \}$ for all $m \in \Z$.
\end{exercise}



\begin{exercise}{7.5.7}
Which subsets of $\R$ are both compact and connected? Justify. \\

Any closed interval in $\R$ will be both compact and connected. That is any set $A= \{ x \in \R : m \leq x \leq n \}$ for some $m, n \in \R$. It is important that this interval not contain any holes, as it is possible to create a compact set with holes which would not be connected. For example,  the Cantor set is considered to be both compact and totally disconnected.
\end{exercise}


\begin{exercise}{7.5.10}
Prove that the closure of a connected set is connected.

\begin{proof}
We need to consider two distinct cases: either $A$ is an open set, or a closed set. If $A$ is a closed set, then $\overline{A} = A$ and hence, the closure is connected. \\
If however $A$ is an open set, then there exists at least one sequence $\{x_n\}$ such that $x_n \in A$ for all $n \in \N$, $\{x_n \} \to L$, but $L \notin A$. Consider now the set $\overline{A}$. Now, $L \in \overline{A}$ and $A \subset \overline{A}$. Assume now that $\overline{A}$ is not a connected set. Then there exists some $x_i$ such that $x_i \in \{x_n\}$ but $x_i \notin \overline{A}$. But by assumption, $x_i \in \{x_n\}$ and therefore $x_i \in A$. Since $A \subset \overline{A}$, which shows that $x_i \in \overline{A}$. Contradiction. Hence, $\overline{A}$ must also be a connected set.
\end{proof}
\end{exercise}





\chapter{Functions on Euclidean Space}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Continuous Functions of Several Variables}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{exercise}{8.1.2}
Give a simple reason why the function $\gamma : \R \to \R^4 $ defined by $\gamma (t) = \left( t, \sin(t), e^t, t^2 \right) $ is continuous in $\R$. \\

The function $\gamma$ consists solely of continuous functions in $\R$, and as such $\gamma(t)$ is likewise continuous (theorem 8.1.5).
\end{exercise}


\begin{exercise}{8.1.4}
Consider the function $f:\R^2  \to \R $ defined by
    \[ f(x,y) = \left\{
     \begin{array}{lr}
       x \, y & : x \, y > 0.\\
       0 & : x \, y \leq 0.
     \end{array}
   \right. \]
   at which points of $\R^2$ is this function continuous? \\


    This function is continuous whenever
    \begin{itemize}
        \item $x,y >0$
        \item $x,y < 0$
        \item $x < 0$, $y > 0$
        \item or $x > 0$, $y < 0.$
    \end{itemize}
    The function is also continuous at zero if we approach $(0,0)$ along the axis from the same quadrant.
\end{exercise}



\begin{exercise}{8.1.5}
For the function  $f : \R^2 \to \R$ defined by
    \[ f(x,y) = \frac{x^2 y}{x^4 + y^2} \]
show that $f$ has a limit $0$ as $(x,y) \to (0,0)$ along any straight line through the origin but that it does not have a limit as $(x,y)\to (0,0)$ in $\R^2$. \\

Assume thatwe approach along the $x = 0$ axis. In that case, the function has the limit $0$. Assume now we approach along some line, which has the form $ y = m x$. Then we consider
    \[ \lim_{x,y \to (0,0)} \frac{x^2  (mx)}{x^4 + (m x^2)^2} = \lim_{x,y \to (0,0)} \frac{x^3 m}{x^4 + m^2 x^4} = \lim_{x,y \to (0,0)} \frac{m x}{x^2 + m^2} = 0.\]
    The function does not however have a limit itself. For a function to have a limit, its limit must be path-independent. This is not the case for the function in question. Assume you approach via the path $y = m x^2$. The resulting limit
        \[ \lim_{x,y \to (0,0)} \frac{x^2 (mx^2)}{x^4 + (mx^2)^2} = \lim_{x,y \to (0,0)} \frac{x^4 m}{x^4 + x^4 m} = \lim_{x,y \to (0,0)} \frac{m}{1 + m}  \]
    clearly depends on $m$, and as such is not path-independent.
\end{exercise}



\begin{exercise}{8.1.12}
Let $B_1(0)$ be the open unit ball in $\R^2$. Is it true that every continuous function $f:B_1(0) \to \R$ takes Cauchy sequences to Cauchy sequences? \\

I can't come up with a counter example; as such, I think this holds true given that we are considering an open ball which includes the point $(0,0)$. If we however choose the go towards the functions via a path of the form $y = m x^2$.
\end{exercise}


\begin{exercise}{8.1.14}
Find a parameterized curve $\gamma(t)$ in $\R^2$, with parameter interval $[0, \infty)$, that begins at $(1,0)$, spirals inward in the counterclockwise direction, and approaches $(0,0)$ as $t \to \infty$. \\

We use the function $f(r,\theta) = \left( r \cos(\theta), r \sin(\theta) \right)$.
\end{exercise}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Properties of Continuous Functions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{exercise}{8.2.3}
If $K$ is a compact, connected subset of $\R^p$ and $f:K \to \R$ is continuous function, what can you say about $f(K)$? \\

We know that a continuous function maps a compact set to a compact set, and that a continuous function also maps a connected set to a connected set. As such, I would argue that $f(K)$ is a compact, connected set.
\end{exercise}



\begin{exercise}{8.2.5}
The image of a compact set under a continuous function is compact, hence closed by theorem 8.2.3. Is the image of a closed set under a continuous function necessarily closed? Prove that it is or give a counter example.  \\

A simple counter example would be the function $f:\R \to \R^+$ where $f(x) = e^x$. Then the preimage of $f$ is closed, yet the image of $f$ is not, as the limit point $\lim_{x \to -\infty} e^x = 0$ is not part of the image set.
\end{exercise}



\begin{exercise}{8.2.7}
Is the sphere $\left\{ (x,y,z) \in \R^3 : x^2 + y^2 + z^2 =1 \right\}$ connected? How do you know? \\

The sphere described above is connected. We know that any path-connected set is a connected set. The surface of a sphere is path connected, as we can join any two points through a continuous path function. As such, I would argue that the sphere is simply connected, which implies that it is a connected set.
\end{exercise}



\begin{exercise}{8.2.10}
Is the function $f: \R^2 \backslash \{ (2,0 \} \to \R$ defined  by
    \[ f(x,y) = \frac{1}{(x-2)^2 + y^2} \]
uniformly continuous on $B_1(0,0)$? Is it uniformly continuous on $B_2(0,0)$? Justify your answers. \\

I think it is uniformly continuous on $B_1 (0,0)$, as well as on $B_2 (0,0)$. In both cases, we can bound the function appropriately such that it goes to 0 without depending on $x$ and $y$.
\end{exercise}



\begin{exercise}{8.2.11}
If $D \subset \R^p$, prove that if a function $F:D \to \R^q$ is uniformly continuous on $D$, then $\{F(x_n)\}$ is a Cauchy sequence in $\R^q$ whenever $\{x_n\}$ is a Cauchy sequence in $D$.

\begin{proof}
    Assume $F$ is defined as above, with $\{x_n\}$ being a Cauchy sqeuence in $D$. Since $F$ is a a uniformly continuous mapping, we know that for all $\epsilon > 0$, there exists some $\delta>0$ such that $\left| f(x_n) - f(x_m) \right| < \epsilon$ whenever $ \left| x_n - x_m \right| < \delta$. Since $\{x_n\}$ is Cauchy, we know that there exists some $N>0$ such that $\left| x_n - x_m \right| < \delta$ whenever $n,m > N$. Due to uniform continuity, we know that $\delta$ is independent of $x$. Hence, if $\left| x_n - x_m \right| < \delta$, we have $\left| f(x_n) - f(x_m) \right| < \epsilon$ for $m,n > N$. As such, the image of the Cauchy sequence is again Cauchy.
\end{proof}
\end{exercise}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Sequences of Functions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{exercise}{8.3.1}
Show that the sequence $\{\gamma_n(t)\}$, where
    \[ \gamma_n(t) = \left( \frac{1}{1+n t}, \frac{t}{n} \right), \]
does not converge uniformly on $[0,1]$. \\

For the function to converge uniformly, both parts of the function need to individually converge uniformly. While it is true that $t/n$ converges uniformly on $[0,1]$ since it can be bound by $1/n$, it is not true that $1/(1+n t)$ converges uniformly. Specifically, we find that
    \[ \left| \frac{1}{1+n} \right| \leq \left| \frac{1}{1+ n t} \right| \leq 1. \]
As can be seen, this bound is dependent on $t$, since we need $n$ large whenever $t$ approaches 0 to be within $\epsilon>0$. Hence, this part of the function fails to converge uniformly, which results in our sequence $\gamma_n (t)$ failing to converge uniformly.
\end{exercise}



\begin{exercise}{8.3.3}
Does the sequence $\{ \left( k^{-1} \sin(kx), k^{-1} \cos(ky)  \right)\} $ converge pointwise in $\R^2$? Does it converge uniformly in $\R^2$? Justify your answers. \\

We can rewrite this sequence as $\left\{ \left( \frac{1}{k \csc(k x)}, \frac{1}{k \sec(k y)}  \right) \right\}$. Clearly, the limit of this sequence is $(0,0)$ for all $x$. As both the secant and cosecant functions aren't properly bounded, I would argue that this convergence cannot be uniform, and must be pointwise.
\end{exercise}




\begin{exercise}{8.3.8}
Does the series $\sum_{k=0}^\infty x^k y^k $ converge uniformly on the square
    \[ \left\{ (x,y) \in \R^2 : -1 < x <1,\,  -1 < y < 1   \right\}?\]
Justify your answer. \\

This series converges uniformly. It is an adaptation of the geometric series in multiple variables. The geometric series exhibits uniform convergence per the Cauchy criterion. As such, I would argue that the extension of the geometric series to $\R^2$ would converge uniformly as well.
\end{exercise}




\begin{exercise}{8.3.10}
Does the series $\sum_{k=0}^\infty \left( x^n, (1-x)^n \right)$ converge pointwise on $[0,1]$? Does it converge pointwise on $(0,1)$? On which subsets of $(0,1)$ does it converge uniformly? Justify your answers. \\

The series does not converge pointwise on $[0,1]$, since $\sum_{k=0}^\infty 1^n$ does not converge. It does however converge pointwise on $(0,1)$, since $\sum_{k=0}^\infty k^n$ converges pointwise provided $|k| < 1$. I would argue that the series in question converges uniformly for all rightclosed subsets of $(0,1)$, i.e. all sets of the form $(0,a]$ where $a<1$. It is evident then that $\sum_{k=0}^\infty x^n$ passes the Weierstrass M-test. We can see though that $\sum (1-x)^n$ will also pass this test. Assume we pick a value $x = \delta>0$. Then we can bound the series $\sum (1- \delta)^n$ with some function $k^n$ such that $k = 1 - \epsilon > 1 - \delta$ for some $\delta > \epsilon > 0$. We are guaranteed such a point exists, and hence this series passes the Weierstrass M-test on all sets of the form $(0,a]$ with $a<1$.
\end{exercise}




\begin{exercise}{8.3.12}
Prove that if $D$ is a subset of $\R^p$ and $\{ F_n\}$ is a sequence of functions from $D$ to $\R^q$, then $\{F_n\}$ fails to converge uniformly to 0 if and only if there is a sequence $\{x_n\}$ in $D$ such that the sequence of numbers $\{ F_n (x_n) \}$ does not converge to 0.

\begin{proof}
    Assume that $\{F_n\}$ does not converge to 0, and that there does not exist a sequence $\{x_n\}$ such that $\{F_n(x_n)\}$ fails to converge to 0. Then for all sequences $\{x_i\} \in D$, we find that $\{F_n ( x_i)\} \to 0$. This means that for all $x$, $\left| F_n (x) \right| < \epsilon$ for some $\epsilon >0$  with $x \in D$ and $n \geq N$ for some $N$. But this is the definition of convergence for$\{F_n\}$. Hence, it cannot be that $\{F_n\}$ fails to converge provided every sequence of $\{x_n\}$ results in $\{F_n (x_n)\}$ going to 0. \\

    Now, assume that $\{F_n\}$ converges to 0, but there exists a sequence $\{x_n\} \in D$ such that $F_n(x_n)$ does not converge to 0. But then, it is not true that $|F_n(x)|< \epsilon$ whenever $x \in D$ and $n \geq N$ for some $N, \epsilon>0$. As such, $\{F_n\}$ would no longer converge. \\

    Therefore, $\{F_n\}$ converges to 0 if and only if for all sequences $\{x_n\} \in D$, $\{ F_n(x_n)\} \to 0$.
\end{proof}
\end{exercise}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Linear Functions, Matrices}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Dimension, Rank, Lines, and Planes}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\chapter{Differentiation in Several Variables}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Partial Derivatives}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Differential}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{exercise}{9.2.1}
	If $L: \R^p \to \R^p$ is a linear function, show that $dL=L$. In other words, if $L$ has a matrix $A$, then $A$ is the differential matrix of the linear function $L(x)=A x$.

	\begin{proof}
    It suffices to show that $\lim_{h \to 0} \frac{L(a+h)-L(a)-A\,h}{|h|}=0$. We need to then show that $L(a+h)-L(a)-A\,h$. But by linearity we can write this is $L(a)+L(h)-L(a)-A\,h =0 $, from which it is evident that $L(a)=A\,h$, and hence, that $\text{d}L=L$.
	\end{proof}
\end{exercise}


\begin{exercise}{9.2.5}
	Find the differential of the real-valued function $f(x,y,z) = x y^2 \cos(x z)$. Then find the best affine approximation to $f$ at the point $(1, 1, \pi/2)$. \\

	The differential is $\text{d} f = \left[\begin{matrix}- x y^{2} z \sin{\left (x z \right )} + y^{2} \cos{\left (x z \right )} & 2 x y \cos{\left (x z \right )} & - x^{2} y^{2} \sin{\left (x z \right )}\end{matrix}\right]$. \\
    % % The best approximation of the function by an affine function when $x$ is near $a$ is given by the affine function $F(a+h) - F(a) - A \,h$, where $A$ is the differential, where $a= (1, 1, \pi/2)$ and $h = (x-1, y-1, z - \pi/2) = (t, u, v)$. We find the approximation
    % \begin{align*}
    % F(a+h) - F(a) - A h &= \begin{bmatrix} F(a_1 + h_1) - F(a_1) - A_1 h_1 \\ F(a_2 + h_2) - F(a_2) - A_2 h_2 \\ F(a_3 + h_3) - F(a_3) - A_3 h_3 \end{bmatrix} \\
    % &= \begin{bmatrix} F(a_1 + h_1) - F(a_1) - A_1 h_1 \\ F(a_2 + h_2) - F(a_2) - A_2 h_2 \\ x y^2 \cos (x z) - (x y^2 \cos(x \pi/2)) - (- x^{2} y^{2} \sin{(x z  )}) (z-\pi/2) \end{bmatrix}
    % \end{align*}
    We find the linear approximation of $f$ at the given point as follows:
    \begin{align*}
    f(x,y,z) &\approx f(1,1,\pi/2) + f_x(1,1,\pi/2) (x-1) + f_y(1,1,\pi/2) (y-1) + f_z(1,1,\pi/2) (z-\pi/2) \\
     &= 0 + \left( \frac{\pi}{2} \left(x - 1\right) \right) + 0 + \left( - z + \frac{\pi}{2} \right) \\
     &= \frac{\pi}{2} - z - \frac{\pi}{2} \left(x - 1\right) = - \frac{1}{2} \left(\pi x + 2 z - 2 \pi\right).
    \end{align*}
    We can see that we can now approximate $f$ by the linear function $L(x,y,z) =- \frac{1}{2} \left(\pi x + 2 z - 2 \pi\right)$. Evaluating $L$ at the point of interest, we see that $L(1,1,\pi/2)=0$.
\end{exercise}


\begin{exercise}{9.2.7}
	Prove that if $f$ is a real-valued function defined on an open subinterval of $\R$ containing $a$ and if $S$ is an affine function such that $f(a) = S(a)$, and
		\[ \lim_{h \to 0} \frac{f(a+h) - S(a+h)}{h} = 0 \]
	then $S(a+h) = f(a) + f'(a) h$.

	\begin{proof}
	Let $S(x)= m\,x + b $. Then
	    \[ f(a+h)  - S(a+h) = f(a+h) - m\,a - m\,h - b. \]
	Since we assume $f(a)=S(a)$, we find then that $ b = f(a) - m \,a $. Then, we rewrite as $f(a+h) - S(a+h) = f(a+h) - f(a) - m\,h $. We find
	  \begin{align*}
	    0 = \lim_{h \to 0} \frac{f(a+h)-S(a+h)}{h} = \lim_{h \to 0} \frac{f(a+h) - f(a)}{h} - \lim_{h \to 0} \frac{m\,h}{h}.
	  \end{align*}
	This implies that $m = f'(a)$ and $S(x) = f'(a) x + f(a) - m\,a$. If we let $x= a+h$, we find
	    \[ S(a+h) = f'(a) (a+h) + f(a) - a\, f'(a) = f(a) + f'(a)h. \]
	This completes the proof.
	\end{proof}
\end{exercise}


\begin{exercise}{9.2.11}
	If $f : \R^p \to \R^p$ is differentiable at $a \in \R^p$, then show that, for each $h \in \R^p$ the function $g: \R \to \R$ defined by $g(t) = f(a + t h)$ has a derivative at $t = 0$. Can you compute it in terms of $d f(a) $ and $h$?

	\begin{proof}
	\emph{MISSING A SOLUTION}
	\end{proof}
\end{exercise}


\begin{exercise}{9.2.12}
	Prove that a function $F: \R^p \to \R^q$ is affine if and only if it is differentiable everywhere and its differential matrix is constant.

	\begin{proof}
    We start by proving that if a function $F: \R^p \to \R^q$ is affine, then it is differentiable everywhere with a constant differential matrix. This is evident when we consider that an affine function $F(x)$ is of the form $F(x) = b + L(x)$, with $L: \R^p \to \R^q$, with an associated matrix $p \times q$-matrix $A$. Also, $b,x \in \R^p$. By exercise 9.2.1, we see that for a linear function $L(x)$ with associated matrix $A$, $\text{d}L=A$. From this we see that $F$ is differentiable everywhere with a constant differential matrix. \\
    We now proceed to consider a function which is differentiable everywhere, and has a constant differential matrix. The function $F = (F_1(x), F_2(x), \hdots, F_p (x) )$. Since the differential is constant, we can write $F_i= a_{1,i} x_1 + a_{2,i} x_2 + \hdots + a_{p,i} x_p + b_i $ for some constants $a_{1,i}, a_{2,i}, ..., b_i$. This implies that $F(x) = b + A x$ where $A=a_{j,i}$ and $b = [b_1, b_2, \hdots, b_q ]^T$. Hence, $F(x)$ is affine.
	\end{proof}
\end{exercise}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Chain Rule}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{exercise}{9.3.1}
	If $F$ is a function from an open subset $U$ of $\R^p$ to $\R^q$ which is differentiable at $a$ and if $B$ is an $r \times q$ matrix, then show that $d( BF) (a) = B d F(a)$. Here, $B F(x)$ is the matrix $B$ applied to the vector $F(x)$ and $BdF(a)$ is the product of the matrix $B$ and the matrix $dF(a)$.

	\begin{proof}
    $F$ is differentiable at $a$ if and only if there exists some function $q(h)$ for $h$ near $0$ such that $F(a+h)-F(a)=Q(h) h$, and $Q(0)=\text{d}F(a)$. Let $Q$ be a function satisfying these conditions. Now let $B$ be some $p\times q$-matrix. Then the following holds:
        \begin{align*}
        B \left(F(a+h)-F(a)\right) &= B\, Q(h)\, h  \\
          B \, F(a+h) - B \, F(a)  &= B\, Q(h)\, h
        \end{align*}
    Note that $B Q(h)$ is continuous at 0 and that $B \, F$ is differentiable at $a$. We then have $\text{d}(B\,F)a = B Q(0) = B\, \text{d} F(a)$.
	\end{proof}
\end{exercise}



\begin{exercise}{9.3.4}
	If $f$ is a differentiable function on $\R$ and $g(x, y) = f(x y)$, show  that $x \frac{\partial g}{\partial x} - y \frac{\partial g}{\partial y} = 0$.

	\begin{proof}
	\emph{MISSING A SOLUTION}
	\end{proof}
\end{exercise}




\begin{exercise}{9.3.6}
	If $u$ is a variable which is a differentiable function of $(x,y)$ in an open set $U \subset \R^2$, if $x$ and $y$ are differentiable function of $(s,t) \in V$ for an open set $V \subset \R^2$, and if $(x,y) \in U$ whenever $(s,t) \in V$, then use the chain rule  to  obtain an expression for $\frac{\partial u}{\partial s}$ and $\frac{\partial u}{\partial t}$ on $V$ in terms of the partial derivatives of $u$ with respect to $x$ and $y$ and the partial derivatives of $x$ and $y$ with respect to $s$ and $t$.

    \begin{align*}
    \frac{\partial d}{\partial s} &= \frac{\partial d}{\partial x} \frac{\partial x}{\partial s} + \frac{\partial d}{\partial y} \frac{\partial y}{\partial s} \\
    \frac{\partial d }{\partial t} &= \frac{\partial d}{\partial x} \frac{\partial x}{\partial t} + \frac{\partial d }{\partial y} \frac{\partial y}{\partial t}
    \end{align*}
\end{exercise}



\begin{exercise}{9.3.8}
	If $F(x,y) = \left( f_1 (x,y), f_2 (x, y) \right)$ is a differentiable function from $\R^2$ to $\R^2$ and if we define $G: \R^2 \to \R^2$ by $G(s, t) = F(s t, s+t)$, find an expression for the differential matrix of $G$ in terms of the partial derivatives of $f_1, f_2$.

	\begin{proof}
	\emph{MISSING A SOLUTION}
	\end{proof}
\end{exercise}



\begin{exercise}{9.3.9}
	If $(x,y,z)$ are the Cartesian coordinates of a point in $\R^3$ and the spherical coordinates of the same point are $r, \theta, \phi$, then
		\begin{align*} &x = r \cos(\theta) \sin(\phi)  &&y = r \sin(\theta) \sin(\phi) &&&z = r \cos(\phi) \end{align*}
	Let $u$ be a variable which is a differentiable  function of $(x,y,z)$ on $\R^3$. Find a formula for the partial derivatives of $u$ with respect to $r, \theta, \phi$ in terms of its partial derivatives with respect to $x,y,z$.

	\begin{align*}
	\frac{\partial u}{\partial r} &= \frac{\partial d}{\partial x} \frac{\partial x}{\partial r} + \frac{\partial d }{\partial y} \frac{\partial y}{\partial r} + \frac{\partial d}{\partial z} \frac{\partial z}{\partial r} \\
	&= \cos(\theta) \sin(\phi) \frac{\partial d}{\partial x} + \sin(\theta) \sin(\phi) \frac{\partial d}{\partial y} + \cos(\phi) \frac{\partial d}{\partial z} \\
	\frac{\partial u}{\partial \theta} &= \frac{\partial d}{\partial x} \frac{\partial x}{\partial \theta} + \frac{\partial d }{\partial y} \frac{\partial y}{\partial \theta} + \frac{\partial d}{\partial z} \frac{\partial z}{\partial \theta}\\
	&= - r \sin{\left (\phi \right )} \sin{\left (\theta \right )} \frac{\partial d}{\partial x} + r \sin{\left (\phi \right )} \cos{\left (\theta \right )} \frac{\partial d}{\partial y}  \\
	\frac{\partial u}{\partial \phi} &= \frac{\partial d}{\partial x} \frac{\partial x}{\partial \phi} + \frac{\partial d }{\partial y} \frac{\partial y}{\partial \phi} + \frac{\partial d}{\partial z} \frac{\partial z}{\partial \phi} \\
	&= r \cos{\left (\phi \right )} \cos{\left (\theta \right )} \frac{\partial d}{\partial x} + r \sin{\left (\theta \right )} \cos{\left (\phi \right )} \frac{\partial d}{\partial y} - r \sin{\left (\phi \right )} \frac{\partial d}{\partial z}
	\end{align*}
\end{exercise}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Applications of the Chain Rule}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{exercise}{9.4.3}
    Find the parametric equation for the tangent line to the curve $\gamma(t) = \left( t^3, 1/t, e^{2t-2}\right)$ at the point where $t=1$. \\

    We note that $\gamma(1) = (1,1,1)$, and $d\gamma(1) = \begin{bmatrix} 3 & -1 & 2 \end{bmatrix}^\intercal$. Then the tangent line $\tau(t)$ can be written as
        \begin{align*}
            \tau(t) &= \begin{bmatrix} 2 \\ 1\\1 \end{bmatrix} + (t-1) \begin{bmatrix} 3 \\ -1 \\ 2 \end{bmatrix} = \begin{bmatrix} 1 \\ 1\\1 \end{bmatrix} + \begin{bmatrix} 3 t - 3 \\ 1 - t\\ 2 t - 2 \end{bmatrix} = \begin{bmatrix} 3t -2 \\ 2- t \\ 2 t - 1 \end{bmatrix}
        \end{align*}
\end{exercise}


\begin{exercise}{9.4.6}
    Show that the gradient  at $x \in \R^p$ of the function $g(x) = x \cdot x$ is the vector $2x$. \\

    We note that we are considering the standard dot product as inner product, such that $g(x) = \langle x, x \rangle = x_2^2 + \hdots x_n^2 = \sum_{i=1}^n x_i^2$. We notice that $\text{deg}(x) = (2 x_1, 2x_2, \hdots, 2 x_n) = 2 x^\intercal$.
\end{exercise}


\begin{exercise}{9.4.11}
    Find the equation of the tangent plane to the cone $z = x^2 + y^2$ at the point $(1,2,5)$. \\

    We see that $\frac{\partial z}{\partial x}= 2x$ and $\frac{\partial z}{\partial y} = 2y$. Then:
    \begin{align*}
    z - z_0 &= \frac{\partial z}{\partial x}(x-x_0) + \frac{\partial z}{\partial y} (y-y_0)\\
    z &= \frac{\partial z}{\partial x}(x-x_0) + \frac{\partial z}{\partial y} (y-y_0 ) + z_0 \\
    z &= 2 x (x - x_0) + 2 y (y-y_0) + z_0 \\
    z &= 2 x (x -1) + 2 y(y-2) + 5.
    \end{align*}
\end{exercise}

\begin{exercise}{9.4.12}
    Show that for each point $(a,b,c)$ on the surface $x^2 + y^2 + z^2 =1$, there is a neighborhood of $(a,b,c)$ in which the surface may be represented as a smoothly parameterized 2-surface. Hence, there is a tangent plane to this surface at every point. \\

    Let $S$ denote the surface given by $x^2 + y^2 + z^2 =1$, which is a sphere. Consider the subsets of S given the graphs of the functions:
    \begin{align*}
        x_+ (y,z) &= \sqrt{ 1 - y^2 - z^2 } && (y^2 + z^2 < 1) \\
        x_- (y,z) &= \sqrt{ 1 - y^2 - z^2 } && (y^2 + z^2 < 1) \\
        y_+ (x,z) &= \sqrt{ 1 - x^2 - z^2 } && (x^2 + z^2 < 1) \\
        y_- (x,z) &= \sqrt{ 1 - x^2 - z^2 } && (x^2 + z^2 < 1) \\
        z_+ (x,y) &= \sqrt{ 1 - x^2 - y^2 } && (x^2 + y^2 < 1) \\
        z_- (x,y) &= \sqrt{ 1 - x^2 - y^2 } && (x^2 + y^2 < 1)
    \end{align*}
    Each of these functions is injective and differentiable. For any $a \in S$, we have that $a$ is in the image of one of these functions, so we can construct a tangent plane to thee surface at any possible point.
\end{exercise}

\begin{exercise}{9.4.13}
    Find an equation for the tangent plane to the surface $x^2 + y^2 -z^2 = 1$ at each point on the surface.  \\

    Let $w = x^2 + y^2 + z^2 $. Then $\nabla w = \left( 2x, 2y, 2z \right)$, which means at point $A = (a,b,c)$ we have $\nabla w(A) = \left( 2a, 2b, 2c \right)$. Then we get the following equation:
    \begin{align*}
    2 a (x -a ) + 2b (y-b) + 2c (z -c ) &= 0\\
    -2 a^2 + 2 ax - 2b^2 + 2 by - 2c^2 + 2 cz &= 0  \\
    2 ax + 2 by + 2cz &= 2 a^2 + 2 b^2 + 2c^2 \\
    ax + by + cz &= a^2 + b^2 + c^2 .
    \end{align*}
\end{exercise}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Taylor's Formula}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{exercise}{9.5.1}
    Find the degree $n =2$ Taylor formula for $f(x,y)=x^2 + x y$ at the point $a=(1,2)$.  \\

    We begin by finding the necessary partial derivatives.
    \begin{align*}
        &\frac{\partial f}{\partial x} = 2x+y, &&\frac{\partial^2 f}{\partial x^2} = 2 \\
        &\frac{\partial f}{\partial y} = x,    &&\frac{\partial^2 f}{\partial y^2} = 1 \\
        &\frac{\partial^f}{\partial x \partial y} = \frac{\partial^2 f}{\partial y \partial x} = 1.
    \end{align*}
    We now use the known equation with these derivatives at the point $a=(1,2)$.
    \begin{align*}
        f(a+x, b+y) &\approx f(a,b) \left( \frac{\partial f(a,b)}{\partial x} + \frac{\partial f(a,b)}{\partial y} y \right) + \frac{1}{2} \left( \frac{\partial^2 f(a,b)}{\partial x^2} x^2 + 2 \frac{\partial^2 f(a,b)}{\partial x \partial y} x y + \frac{\partial^2 f(a,b)}{\partial y^2} y^2 \right)  \\
                    &= 3 + \left(4x+y\right) + \frac{1}{2} \left( 2 x^2 + 2 xy + y \right)  \\
                    &= \frac{1}{2} \left( 2x^2 + 2xy+ 8x+y^2 + 2 y + 6\right).
    \end{align*}
\end{exercise}

\begin{exercise}{9.5.4}
    Suppose $U$ is an open convex set and $f$ is a differentiable real-valued function on $U$. If there is a number $M > 0$ such that $|df(x)| \leq M$ for all $x\in U$, then
        \[ |f(x) - f(y) \leq M |x-y| \]
    for all $x,y \in U$.

    \begin{proof}
        Let $x,y \in U$. Then by Taylor's theorem we have $f(x) = f(y) + df(c) (x-y)$ for some $c \in [y,x]$. Then by algebra:
        \begin{align*}
            |f(x) - f(y) | &= |df(c)(x-y)| \\
                           &\leq |df(c)| |x-y| \\
                           &\leq m | x-y|.
        \end{align*}
        This completes the proof.
    \end{proof}
\end{exercise}


\begin{exercise}{9.5.6}
    Show that the following form of the Mean Value Theorem is not true: if $F:\R^2 \to \R^2$ is a differentiable function and $a,b \in \R^2$, then there is a $c$ on the line segment joining $a$ to $b$ such that $F(b) - F(a) = \text{d} F(c) (b-a)$. The problem here is that $F$ is vector-valued, not real valued.  \\

    If the function $F$ is vector-valued as above, we cannot make sense of the equation. Specifically, we find that $\text{d} F(c) (b-a)$ is defined as a scalar multiplication. In this case, the result is a scalar, whereas $F(b)-F(a)$ would be a case of vector addition. A scalar cannot equal a vector. Alternatively, we could attempt to solve this issue by redefining $\text{d} F(c) (b-a)$ as matrix multiplication. However, then we could only have equality in the case of $\text{d} F$ begin a square matrix; otherwise, the dimensions wouldn't match up. As such, it appears as though we can't make sense of the equation as is for vector valued functions.
\end{exercise}



\begin{exercise}{9.5.8}
    Find all points of relative maximum and relative minimum and all saddle points for $f(x,y) = 1 - 2 x^2 - 2 x y- y^2$.  \\

    We begin by finding the differential $\nabla f(x,y) = \left( \frac{\partial f}{\partial x}, \frac{\partial f}{\partial y} \right) = \left( -4x-2y, -2x-2y \right)$. This is only zero at the point $(0,0)$. We calculate the Discriminant at this point:
        \[ D = (-4)(-2)- (-2)^2 = 8 -4 = 4. \]
    Since $D>0, \, \frac{\partial^2 f}{\partial x^2} < 0$ we find that we are dealing with a point of local maximum.
\end{exercise}


\begin{exercise}{9.5.9}
    Find all  points of relative maximum and relative minimum and all saddle points for $f(x,y) = y^3 + y^2 +x^2 - 2 x y -3y$.  \\

    We begin by taking the various partial derivatives:
    \begin{align*}
        &\frac{\partial f}{\partial x} = 2x-2y, &&\frac{\partial^2 f}{\partial x^2} = 2 \\
        &\frac{\partial f}{\partial y} = 3y^2+2y-2x-3, &&\frac{\partial^2 f}{\partial y^2}=6y+2\\
        &\frac{\partial^2 f}{\partial x \partial y} = \frac{\partial^2 f}{\partial y \partial x} = -2
    \end{align*}
    We then find the points where $\nabla f= \vec{0}$. As $\nabla f= \left( 2x-2y, 3y^2+2y-2x-3\right)$, we find that we have critical points at $(-1,-1)$ and $(1,1)$. \\
    At the point $(-1,-1)$ we have $D=2 (6(-1)+2) - (-2)^2= -12$, and therefore we are dealing with a saddle point here.\\
    At the point $(1,1)$ we have $D=2(6+2)-(-2)^2=4$, and hence we have a point of local minimum here.
\end{exercise}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Inverse Function Theorem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{exercise}{9.6.2}
    Show that the function $F: \R^2 \to \R^2$ defined by $F(x,y) = (x^2+y^2, x y)$ has a smooth local inverse near points $(x,y)$ where $x \neq \pm y$. Find the inverse function $F^{-1}$ on the set $\{(x,y) : -x < y < x\}$ and identify its domain. Calculate the differential of this inverse function (1) directly and (2) by using the Inverse Function Theorem. Verify that the two methods give the same answer.  \\

    Let $u=x^2+y^2$ and $v = x y$. Then we find that $u+2v = (x+y)^2$ and $u-2v=(x-y)^2$. On the set $\{(x,y) : -x < y < x\}$ we find that $x+y>0$ and $x-y>0$. This means that $J_F (x,y) = 2 (x^2-y^2)\neq 0$ and we have an inverse. We find then that
    \begin{align*}
        &x+y=\sqrt{u+2v}        &&x-y=\sqrt{u-2v}
    \end{align*}
    From this we get that
    \begin{align*}
        x &= \frac{1}{2} \left( \sqrt{u+2v} + \sqrt{u-2v} \right) \\
        y &= \frac{1}{2} \left( \sqrt{u+2v} - \sqrt{u-2v} \right)
    \end{align*}
    We calculate the differential:
    \begin{align*}
    \begin{bmatrix} \frac{\partial x}{\partial u} & \frac{\partial x}{\partial v} \\ \frac{\partial y}{\partial u} & \frac{\partial y}{\partial v} \end{bmatrix} &= \left[\begin{matrix}\frac{1}{4 \sqrt{u + 2 v}} + \frac{1}{4 \sqrt{u - 2 v}} & \frac{1}{2 \sqrt{u + 2 v}} - \frac{1}{2 \sqrt{u - 2 v}}\\\frac{1}{4 \sqrt{u + 2 v}} - \frac{1}{4 \sqrt{u - 2 v}} & \frac{1}{2 \sqrt{u + 2 v}} + \frac{1}{2 \sqrt{u - 2 v}}\end{matrix}\right]
    \end{align*}
    Now according to the inverse function theorem:
    \begin{align*}
        \text{d} F^{-1} (b) &= \left( \text{d}F(a) \right)^{-1} \\
                            &= \left( \begin{bmatrix} \frac{\partial (x^2+y^2)}{\partial x} & \frac{\partial (x^2+y^2}{\partial y} \\ \frac{\partial(xy)}{\partial x} & \frac{\partial(xy)}{\partial y} \end{bmatrix} \right)^{-1} \\
                            &= \left( \begin{bmatrix} 2x & 2y \\ y  & x \end{bmatrix}  \right)^{-1} \\
                            &= \left[\begin{matrix}\frac{x}{2 x^{2} - 2 y^{2}} & - \frac{y}{x^{2} - y^{2}}\\- \frac{y}{2 x^{2} - 2 y^{2}} & \frac{x}{x^{2} - y^{2}}\end{matrix}\right] && \text{and substituting:} \\
                            &= \left[\begin{matrix}\frac{1}{4 \sqrt{u + 2 v}} + \frac{1}{4 \sqrt{u - 2 v}} & \frac{1}{2 \sqrt{u + 2 v}} - \frac{1}{2 \sqrt{u - 2 v}}\\\frac{1}{4 \sqrt{u + 2 v}} - \frac{1}{4 \sqrt{u - 2 v}} & \frac{1}{2 \sqrt{u + 2 v}} + \frac{1}{2 \sqrt{u - 2 v}}\end{matrix}\right]
    \end{align*}
\end{exercise}




\begin{exercise}{9.6.5}
    Find a smooth local inverse function near $(1, \pi/2)$ for the function $F$ of Example 9.6.6.  \\

We find that $\text{d} F(r, \theta)= \begin{pmatrix} \cos(\theta) & -r \sin(\theta) \\ \sin(\theta) & r \cos(theta) \end{pmatrix}$. The determinant is non-zero whenever $r\neq 0$. At the point $F(a)=(0,1)$. We know by the inverse function theorem that $\text{d} F^{-1} (b) = \left( \text{d} F(a) \right)^{-1}$. We now use this information and find the inverse differential:
    \begin{align*}
    \text{d} F^{-1}(b) = \left( \text{d} F(a)\right)^{-1} = \begin{bmatrix} \cos(\theta) & \sin(\theta) \\ -r^{-1} \sin(\theta) & r^{-1} \cos(\theta) \end{bmatrix} = \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}.
    \end{align*}
    We find that the inverse function is $F^{-1}(x,y) = \left( \sqrt{x^2 + y^2}, \tan^{-1}(y/x)\right)$.
\end{exercise}





\begin{exercise}{9.6.8}
    Show by example that the result of the previous problem is not true if $U$ is only assumed to be connected, rather than convex. Hint: Try the function $F(x,y) = (x^2 - y^2, 2 x y)$ on $\R^2 \backslash \{0\}$.  \\

    Consider the function $f(x,y) = (x^2-y^2, 2xy) $. Then $df = \begin{bmatrix} 2x && -2y\\ 2y && 2x \end{bmatrix} $. Then the determinant of $df$ is greater than 0, namely it is $4(x^2+y^2)$. Consider now $U=\R^2 \backslash \{(0,0)\}$, which is a connected but not a convex set. Here, $df$ is non-singular but $f$ is not injective since $f(1,-1)=f(-1,1)$.
\end{exercise}





\begin{exercise}{9.6.10}
    Show that the condition that $d F(a)$ be non-singular is necessary in the Inverse Function Theorem by showing that if a function $F$ from a neighborhood of $a$ in $\R^P$ to $\R^p$ is differentiable at $a$ and has an inverse function at $a$ which is differentiable at $F(a)$, then $d F(a)$ is non-singular.  \\

    We are trying to show that if $F$ is locally invertible, its differential matrix is also invertible, i.e. non-singular. \\
 Assume the inverse of $F$ exists, and refer to it as $F^{-1}$, such that $F \circ F^{-1} = I$. If we differentiate, we find that we are looking for a function such that
    \[ I = \text{d} F \, \text{d} F^{-1} \left( F \right) . \]
    By linear algebra, this implies that $( \text{d} F(a) )^{-1}$ is the inverse differential of $\text{d} F(a)$, which implies that $\text{d} F(a)$ is non-singular.
\end{exercise}




\begin{exercise}{9.6.12}
    If $F: \R^p \to \R^p$  is a $C^1$ function, what can you say about $F$ at a point of $\R^p$ where $|F|$ has a local minimum? How about a point where $|F|$ has a local maximum?  \\

    The norm of a function $F$ is defined as $|F| = \sqrt{ (F_1)^2 + \hdots + (F_p)^2}$ for a function with $p$-components. Differentiating both sides with respect to some variable $x$ yields the equation
    \[ 2 |F| \frac{d |F|}{x} = 2 F_1 \frac{d F_1}{d x} + \hdots + 2 F_p \frac{d F_p}{d x} \]
    We note that since we are dealing with local minima/maxima, that $\frac{d |F|}{x}=0$. As such, we get the equation
    \[ 0 = \sum_{i=1}^p F_i \frac{d F_i}{dx}. \]
    This is an interesting equation.
\end{exercise}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Implicit Function Theorem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{exercise}{9.7.1}
    Are there any points on the graph of the equation $x^3 +3xy+2y^3=1$ where it may not be possible to solve for $y$ as a smooth function of $x$ in some neighborhood of the point?  \\

Consider the function $f(x,y)=x^3 + 3xy^2+2y^3 -1$. By the implicit function theorem, we can solve this function for $y$ as a smooth function of $x$ whenever $\frac{\partial f}{\partial y}\neq 0$. We find that $\frac{\partial f}{\partial y}= 6xy + 6 y^2$. Solving this for zero is equivalent to solving $y(x+y)=0$.  Therefore, we can solve for $y$ as a smooth function of $x$ at all points where $y \neq 0$ and $y \neq -x$.
\end{exercise}


\begin{exercise}{9.7.3}
    Find $\frac{\partial(f_1,f_2)}{\partial(u,v)}$ if
        \begin{align*}
            f_1(x,y,u,v) &= u^2 + v^2 + x^2 + y^2, \\
            f_2(x,y,u,v) &= xu + yv + x - y.
        \end{align*}
    At which points  $(x,y,u,v)$ is this matrix non-singular?  \\

    We solve for the differential:
    \begin{align*}
        \frac{\partial(f_1, f_2)}{\partial (u,v)} = \begin{bmatrix} \frac{\partial f_1}{\partial u} & \frac{\partial f_1}{\partial v} \\ \frac{\partial f_2}{\partial u} & \frac{\partial f_2}{\partial v} \end{bmatrix} = \begin{bmatrix} 2 u & 2 v \\ x & y \end{bmatrix}
    \end{align*}
    We find that this matrix is singular whenever $2 u v= 2 vx$. Hence, this matrix is non-singular for all points $(x,y,u,v) \in S = \{(x,y,u,v) : 2 u v \neq 2 vx \}$.
\end{exercise}


\begin{exercise}{9.7.4}
    Show that the system of equations
    \begin{align*}
        u^2 + v^2 + 2 u - x y + z &= 0 \\
        u^3 + \sin(v) - x u + yv + z^2 &= 0
    \end{align*}
    has a solution for $(u,v)$ as a smooth function of $(x,y,z)$, in some neighborhood of $(0,0,0)$, with the property that $(u,v) = (0,0)$ when $(x,y,z) = (0,0,0)$.  \\

    Let $f_1 = u^2 + v^2 + 2 u - x y + z $ and $f_2 = u^3 + \sin(v) - x u + yv + z^2 $. We solve for the differential at $(0,0,0,0,0)$:
    \begin{align*}
        \frac{\partial f}{\partial (u,v)} &= \begin{bmatrix} 2 u + 2 & 2v \\ 3u^2-x & \cos(v)+y \end{bmatrix} \\
        \frac{\partial f}{\partial (u,v)} (0,0,0,0,0) &= \begin{bmatrix} 2 & 0 \\ 0 & 1 \end{bmatrix}
    \end{align*}
    As the last differential is if non-singular, this is possible.
\end{exercise}


\begin{exercise}{9.7.6}
    For the equation $xy + yz + xz = 1$, at which points on the solution set $S$ is there a neighborhood in which $S$ is a smooth 2-surface? At each such point $(a,b,c)$, find an equation of the tangent plane.  \\

    This function describes a two sheeted hyperbola. Let $F(x,y,z):  xy + yz + xz =1$.  THen $\nabla F = (y+z, x+z, y+x)$. We find that $\nabla F = (0,0,0)$ only at the points $(0,0,0)$ and $(x,-x,0)$. Of these, the only points on the graph are $(1,-1,0)$ and $(-1,1,0)$. We find that $F(1,-1,0)=F(-1,1,0)$. This means that there exists a neighbord $V$ of $(1,-1,0)$ and $(-1,1,0)$ with $V \subset \R^3$ there exists a level set $S = \{u \in V : F(u)=0\}$ which is a smooth p-surface. The tangent space at $(1,-1,0)$ and $(-1,1,0)$ consists of the set of the solutions $u$ to $\nabla F(1,-1,0) (u - (1,-1,0))=0$ and $\nabla F(-1,1,0) (u - (-1,1,0))=0$.
\end{exercise}


\begin{exercise}{9.7.8}
    For the system of equations
    \begin{align*}
        x^2 + y^2 + u^2 - 3 v &= 1, \\
        2 x + x y - y + 3 u^2 - 9 v &= 0,
    \end{align*}
    find all points on the solution set $S$ for which there is a neighborhood in which $S$ is a smooth 2-surface.  \\

    Let $f_1 = x^2+y^2+u^2-3v-1$ and $f_2= 2x + xy - y + 3u^2 - 9v$. We find the differential
    \begin{align*}
    \frac{\partial (f_1,f_2)}{\partial(x,y,u,v)} &= \begin{bmatrix} \frac{\partial f_1}{\partial x} & \frac{\partial f_1}{\partial y} & \frac{\partial f_1}{\partial u} & \frac{\partial f_1}{\partial v} \\ \frac{\partial f_2}{\partial x} & \frac{\partial f_2}{\partial y} & \frac{\partial f_2}{\partial u} & \frac{\partial f_2}{\partial v}  \end{bmatrix} \\
                                                 &= \begin{bmatrix} 2x & 2y & 2u & -3 \\ 2+y & x-1 & 6u & -9 \end{bmatrix}
    \end{align*}
    From this we can see that we can express the system of equations in terms of $x,y$ as functions of $u,v$ provided that $x \neq \pm \sqrt{ (y+1)^2}$.

\end{exercise}






\chapter{Integration in Several Variables}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Integration over a Rectangle}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Jordan Regions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Integral over a Jordan Region}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Iterated Integrals}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Change of Variables Formula}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\chapter{Vector Calculus}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{1-forms and Path Integrals}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{exercise}{11.1.1}
    Find a smooth curve in $\R^2$ which traces the straight line from $(1,2)$ to $(3,0)$.\\

    The solution is the function $\gamma(t) = \vec{u} + t(\vec{v}-\vec{u})$ where $\vec{u}=(1,2)$, $\vec{v}=(3,0)$, and $t \in [0,1]$. We rewrite:
    \begin{align*}
    \gamma(t) &= \begin{bmatrix} 1 \\ 2 \end{bmatrix} + t \left(\begin{bmatrix} 3 \\ 0 \end{bmatrix}- \begin{bmatrix} 1 \\ 2 \end{bmatrix} \right) \\
              &= \begin{bmatrix} 1 \\ 2 \end{bmatrix} + t \begin{bmatrix} 2 \\ -2 \end{bmatrix} = \begin{bmatrix} 1 + 2 t\\ 2 - 2 t \end{bmatrix}
    \end{align*}
\end{exercise}


\begin{exercise}{11.1.2}
    We find the derivative $\gamma ' (t) = \left( \cos(t)-t \sin(t), \sin(t) + t \cos(t) \right)$. We proceed to find the norm
    \[ \left| \gamma'(t) \right| = \sqrt{ \left( \cos(t) - t \sin(t)\right)^2 + \left( \sin(t) + t \cos(t) \right)^2 } = \sqrt{1+t^2}. \]
    To find the length, we solve the integral $\int_0^{4 \pi} \sqrt{1+t^2} dt$. We utilize a trig substitution, lettting $t = \tan(\theta)$.
    \begin{align*}
        \int_0^{4 \pi} \sqrt{1+t^2} dt &= \int_0^{\tan^{-1}(4 \pi)} \sec(\theta) d\theta \\
                                       &= \left. \ln \left( \sec(\theta) + \tan(\theta) \right) \right|_0^{\tan^{-1}(4 \pi}) \\
                                       &= \ln \left( \sec\left( \tan^{-1}(4 \pi)\right) + 4 \pi \right) = \ln \left( 4 \pi + \sqrt{16 \pi^2 +1 }\right)
    \end{align*}
    The last step is justified by letting $\sec\left( \tan^{-1}(4 \pi)\right)= \sec (\theta)$, and solving $\tan^2 (\theta) + 1 = \sec^2(\theta)$.
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Change of Variables}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Differential Forms of Higher Order}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Green's Theorem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Surface Integrals and Stoke's Theorem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Gauss's Theorem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Chains and Cycles}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




% --------------------------------------------------------------
%     You don't have to mess with anything below this line.
% --------------------------------------------------------------

\end{document}
